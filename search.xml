<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>Andrew Ng&#39;s development advice for students in AI field</title>
      <link href="/2022/01/02/%E5%90%B4%E6%81%A9%E8%BE%BE/"/>
      <url>/2022/01/02/%E5%90%B4%E6%81%A9%E8%BE%BE/</url>
      
        <content type="html"><![CDATA[<h1 id="关于如何阅读文献"><a href="#关于如何阅读文献" class="headerlink" title="关于如何阅读文献"></a>关于如何阅读文献</h1><font size=4>深度学习领域的发展真的很快，以至于即使你已十分熟悉深度学习的基础内容，还是很难做到在理论和实践知识上都与行业与时俱进，因此，能否有效阅读文献对开发和研究者们来说可谓是尤其重要，我可以给你们一些参考建议。通常我们都会觉得“学会怎么读论文”是一个潜移默化的过程，但我希望这次分享内容能帮你们加速这一过程。<p>现在，假设你想通过阅读文献来增近对某一特定领域的了解，比如自然语言处理，我会采取的策略是<strong>先汇总一份需要阅读的论文清单</strong>，这些论文的来源可以是 arXiv，互联网，Medium 博文，或者 GitHub 上的一些帖子。</p><p>但不管你的学习资源来自哪里，<strong>在你汇总出一份文献清单后，我推荐“并行+扩充阅读法”，而非按清单从上往下，一篇篇从头读到尾。</strong></p><p><strong>（清单+百分比，速读）</strong><br>首先，画一个表格，表格的纵轴上列出你待读清单上的论文，横轴上是一篇论文的读完百分比（从百分之零到百分之百），然后对所有的论文进行速读，<strong>大概掌握每篇论文要讲什么</strong>，将每篇论文的阅读进度都完成到约百分之十。</p><p><strong>（判断meanless与具有key massage的Paper，花费不同的时间与精力）</strong><br>然后在此基础上，如果我们能判断出清单上有那种“意义不大”的论文，即使有许多论文都引用了它，也不要害怕说自己判断错了，要大胆相信自己的判断，将它移出清单，而除此之外，我们通常也能在这一过程中判断出，哪些论文是那些具有关键信息的文献，而一旦识别出了这些文献，我们便要花更多的时间和精力去细读它们。</p><p><strong>（按照一种“树形”结构去理解重点且基础文献，注意树枝不要太长）</strong><br>如此一来，我们便有可能在这一细读的过程中又发掘出一些新的文献，然后在发觉到“要理解当下细读文献的内容，需要先读新发掘出的文献”时，我们就要先去阅读那篇新发掘出的更为基础的文献，然后再在阅读这篇新发掘出的文献时重复刚才的过程，直到找到一篇能直接读到底的文献，将它读完后，再回去将之前引出这篇“底部文献”的那些文献按顺序一一读完。</p><p>而就论文的阅读数量来说，<strong>一般需要读 15 到 20 篇论文，才能对一个细枝领域有一些基本的了解</strong>，并也只有是在这一基础上，才能进行一些真正意义上的实践，在有理解的基础上实现一些算法。</p><p><strong>通常，在读了 50 到 100 篇论文后，能对想要了解的某一细枝领域有一个相对较好的了解</strong>。而如果只是读 5 到 20 篇文献，我们或许能实现一些算法，但没法跟上该领域的前沿进展。</p><p>个人来说，我在阅读一篇论文时，通常会将阅读的过程分为几步。</p><ul><li>第一步是读论文的<strong>标题、摘要和图表</strong>。</li></ul><p>图表们在深度学习领域的论文中尤为重要，很多论文都会用图表来概括整篇论文所讲的内容。取决于具体的需要，<strong>我或许还会略读一下论文的方法和实验部分</strong>，然后通过以上所有，我就能在不细读整篇论文的情况下，较好地了解论文是要讲什么。</p><ul><li>在第一步的基础上，我会开始第二步，<strong>细读论文的 Introduction（介绍）和结论，并细读这两个部分中所使用到的图表。</strong></li></ul><p>而对于论文中的 <strong>Related Work（相关研究）部分</strong>，很多人会在第一次阅读时略过，原因是如果在开始阅读前对论文所涉及的领域了解不深，可能根本读不懂这里讲的是什么，但如果我们决定要阅读这一部分，需要<strong>注意这一部分内容客观性</strong>，很多作者在书写这一部分时，都是选那些能支持论文内容的材料写进来，作为“说服审稿人发表这篇论文”之努力的一部分，所以这里要有这个意识，注意这个部分的客观性。</p><ul><li><p>在进行完第二步后，第三步的内容非常简单，就是<strong>先略过理论的数学部分</strong>来读论文，然后我们便可以进行下一步了。</p></li><li><p>第四步，<strong>通读全文</strong>，<strong>但要在阅读过程中略过讲不通的部分</strong>，原因是实际发表论文时，都会尝试将论文包装成一种前沿研究，而有时我们其实并不知道所写的论文里哪些重要，哪些可能其实不重要。</p></li></ul><p>因此，在很多被大量引用的论文中，大家在后来都是发现说这些论文的部分内容十分有用，而有的部分则是根本不重要，但重点是，作者在写论文的时候是没法知道这些的，通常没法在写论文时就知道所写的内容里哪些比较重要，哪些不重要。</p><p>比如 Yann LeCun 的 LeNet 系列论文，论文中有的内容成为了卷积神经网络发展的基石，有的内容则是一些不太相干的东西，比如 Transducers 等概念，这些在现在很少有人会用。</p><p>所以，当你在阅读一篇论文时觉得<strong>“有的东西讲不通，或者根本没什么用”时，不用怀疑自己，跳过就行了</strong>。当然，如果你想做关于某一方面的深度研究，那当我没说。</p><p>一般来说，我个人按以上方法读完一篇论文需要大概 30 分钟，但我在机器学习领域算是比较专业了，掌握的信息很多，所以能在大概半小时内读完，你们可以按照自己的速度来，不用参照我的时间。<br></font></p><h1 id="关于文献的来源"><a href="#关于文献的来源" class="headerlink" title="关于文献的来源"></a>关于文献的来源</h1><font size=4>另外一个我经常会被问到的，关于阅读文献的问题。<p>一般去哪里搜集需要阅读的文献？关于这一点，我想说的是，上网检索是非常重要的。关于一个话题的相关信息，无论是论文还是技术博文，你都能通过网上检索找到。</p><p>此外，很多人都想做到说，自己掌握的信息能否“实时”与深度学习这一领域的前沿发展接轨，关于这一点，我觉得<strong>T特</strong>现在已经成了科研人员们用于发现新事物的一个好地方，除此以外，在 <strong>Reddit</strong> 上关注机器学习话题也会有所帮助。当然，关注<strong>领域内的会议</strong>作用也很大，比如 NIPS、ICML 和 ICLR。<br></font></p><h1 id="关于深度阅读文献"><a href="#关于深度阅读文献" class="headerlink" title="关于深度阅读文献"></a>关于深度阅读文献</h1><font size=4><p>关于阅读文献我还想再补充两点：</p><ul><li>很多论文或其它资料里都有<strong>数学内容</strong>，有时这些数学内容里的推导可能非常难懂，而如果你想确保自己有读懂这些内容，我的建议是<strong>先通读一遍，然后看自己能不能从论文中给出的推导的起点，推出后面的那些算式，如果你能成功完成推导的话，那你绝对是已经完全读懂了这篇论文。</strong></li></ul><p>而就我个人来说，由于我在读博士时就经常这么做，我发现这种做法后来<strong>除了能帮我读懂别人的研究，还有锻炼发现新算法的能力。</strong></p><p>当然，这么做会需要你花费大量的时间，要不要这么做还是取决于你的具体需求，比如你是否真的想要完全读懂一篇论文。</p><ul><li>关于如何读懂<strong>代码</strong>，与数学部分类似，我的建议是，<strong>如果你想完全搞懂这篇论文所用的代码，就尝试在看完代码以后，重新实现一遍论文所用的代码，如果能成功实现的话，便说明你已经完全搞懂论文所用的代码了。</strong></font></li></ul><h1 id="关于文献阅读习惯的长期建议"><a href="#关于文献阅读习惯的长期建议" class="headerlink" title="关于文献阅读习惯的长期建议"></a>关于文献阅读习惯的长期建议</h1><font size=4><p><strong>在长期上，我建议常读论文，而不是采取那种突击式读论文的方法</strong>，比如可以每周读一两篇，而不是在感恩节假期时突击读它个 50 篇然后就再也不读了。<br>从教育学和脑科学上来说，这么做也能帮我们养成一个良好的习惯。<br></font></p><h1 id="关于职业发展"><a href="#关于职业发展" class="headerlink" title="关于职业发展"></a>关于职业发展</h1><font size=4>之前很多学生都有问我要一些关于职业发展的建议，这也确实是一个很重要的问题，比如就我们所知，现在机器学习所涉及的领域真的是太广了，如何才能知道自己究竟想做什么呢？<p>首先，我需要做一个假设，就是大多数人，都只想在职业发展上涉及一到两个领域。</p><p>无论你是想留在大学里，还是去公司里工作，我都希望你们所从事的工作是重要的，或者说，是有意义的，然后在此基础上，我想谈两个与职业发展有关的问题，一是要怎么拿到一个职位（Phd，教职或是公司职位），二是我们要如何着眼于长期规划来选择职位。</p><p><strong>1、关于找工作，招聘官们在寻找什么样的人？</strong></p><p>首先，很多招聘官们都会青睐那些<strong>有专业技能的人</strong>，比如在机器学习领域，很多面试官会问你，你会不会用这种方法或是那种方法，然后再细问一些关于特定方法的问题，比如，在使用 batch gradient descent 这一方法时，调整 mean batch size 会有什么样的影响等等。然后，除了你在机器学习上的知识，很多面试官也会关注你的编程能力。</p><p>此外，他们还会关注“你之前做过哪些有意义的工作”，原因是一个人有很棒的理论知识基础，并不能说明这个人就能很好的使用这些理论，所以，<strong>如果能有一些理论实践的话将是很好的。</strong></p><p>许多面试官也会看重面试者的<strong>持续学习能力。</strong></p><p>很多工作并不需要你是个“全通”，你只需在对机器学习有一个整体的了解上，能做到相对精通那份工作会涉及到的领域的内容就行了。</p><p>关于这个有很多衡量标准，有时面试官可能会想要了解你此前在有关领域内的工作，或者是你此前在有关领域内产出的一些开源代码，这些信息能帮助面试官判断<strong>“你是否能就一个问题给出有效的解决方案”。</strong></p><p><strong>2、如何选择一份工作</strong></p><p><strong>如果你的目标是想做到持续学习，和厉害的人从事有意义的工作是很重要的，环境的熏陶有时还是能带来蛮大影响的。</strong></p><p>所以，在选择自己想要去哪里工作时，要注意那里的团队怎么样，是否是自己想要的一个环境。</p><p>还有就是，<strong>你是否觉得那份工作的上级与你合得来。</strong></p><p>最后就是<strong>公司品牌</strong>。比起品牌，你更需要关注的是你是否能与可能的上司合得来，是否能与团队合得来，是否喜欢那里的工作环境，这一点对于很多招聘官来说也是一样的，<strong>招聘和面试官们比起你之前在哪工作过，会更关注你是个什么样的人。</strong></p><p>另外，如果一份工作在你收到 offer 后拒绝给你透露你将要进行的项目内容、团队和上级信息，要提高警惕。因为这种情况下，如果没有特殊原因，意味着将这些信息透露给你将会降低这份工作对你的吸引力。</p><p>最后，还是之前谈到的，可能是最重要的一个建议，在选择工作时，<strong>选择那些你觉得有意义的工作，并在工作中积极学习你能学到的东西。</strong></p><p>我个人觉得，未来，AI 并不是只有在这些科技公司里才能用的上，而是在很多被视作“传统行业”的领域里都能大显身手，所以在职业选择上，<strong>我建议不用把自己限制在科技行业里。</strong><br></font></p>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>First principles thinking</title>
      <link href="/2021/12/24/First%20principles%20thinking/"/>
      <url>/2021/12/24/First%20principles%20thinking/</url>
      
        <content type="html"><![CDATA[<p> <font face=黑体 size=4>第一性原理（First principle thinking，又称“第一原理”）</p><p>古希腊哲学家亚里士多德：在每一系统的探索中，存在第一原理，是一个最基本的命题或假设，不能被省略或删除，也不能被违反。 ”</p><p>下面片段节选自James Clear的《Atomic Habits》:</font></p><p> <font face=Georgia size=10>F</font><font face=Georgia size=5>irst principles thinking, which is sometimes called reasoning from first principles, is one of the most effective strategies you can employ for breaking down complicated problems and generating original solutions. It also might be the single best approach to learn how to think for yourself.</p><p>The first principles approach has been used by many great thinkers including inventor Johannes Gutenberg, military strategist John Boyd, and the ancient philosopher Aristotle, but no one embodies the philosophy of first principles thinking more effectively than entrepreneur Elon Musk.</p><p>In 2002, Musk began his quest to send the first rocket to Mars—an idea that would eventually become the aerospace company SpaceX.</p><p>He ran into a major challenge right off the bat. After visiting a number of aerospace manufacturers around the world, Musk discovered the cost of purchasing a rocket was astronomical—up to $65 million. Given the high price, he began to rethink the problem.</p><p>“I tend to approach things from a physics framework,” Musk said in an interview. “Physics teaches you to reason from first principles rather than by analogy. So I said, okay, let’s look at the first principles. What is a rocket made of? Aerospace-grade aluminum alloys, plus some titanium, copper, and carbon fiber. Then I asked, what is the value of those materials on the commodity market? It turned out that the materials cost of a rocket was around two percent of the typical price.”</p><p>Instead of buying a finished rocket for tens of millions, Musk decided to create his own company, purchase the raw materials for cheap, and build the rockets himself. SpaceX was born.</p><p>Within a few years, SpaceX had cut the price of launching a rocket by nearly 10x while still making a profit. Musk used first principles thinking to break the situation down to the fundamentals, bypass the high prices of the aerospace industry, and create a more effective solution.</p><p>First principles thinking is the act of boiling a process down to the fundamental parts that you know are true and building up from there. Let’s discuss how you can utilize first principles thinking in your life and work.</font></p><h1 id="Defining-First-Principles-Thinking"><a href="#Defining-First-Principles-Thinking" class="headerlink" title="Defining First Principles Thinking"></a>Defining First Principles Thinking</h1><p><font face=Georgia size=5>A first principle is a basic assumption that cannot be deduced any further. Over two thousand years ago, Aristotle defined a first principle as “the first basis from which a thing is known.”</p><p>First principles thinking is a fancy way of saying “think like a scientist.” Scientists don’t assume anything. They start with questions like, What are we absolutely sure is true? What has been proven?</p><p>In theory, first principles thinking requires you to dig deeper and deeper until you are left with only the foundational truths of a situation. Rene Descartes, the French philosopher and scientist, embraced this approach with a method now called Cartesian Doubt in which he would “systematically doubt everything he could possibly doubt until he was left with what he saw as purely indubitable truths.”</p><p>In practice, you don’t have to simplify every problem down to the atomic level to get the benefits of first principles thinking. You just need to go one or two levels deeper than most people. Different solutions present themselves at different layers of abstraction. John Boyd, the famous fighter pilot and military strategist, created the following thought experiment which showcases how to use first principles thinking in a practical way.</p><p>Imagine you have three things:</p><p>A motorboat with a skier behind it<br>A military tank<br>A bicycle<br>Now, let’s break these items down into their constituent parts:</p><p>Motorboat: motor, the hull of a boat, and a pair of skis.<br>Tank: metal treads, steel armor plates, and a gun.<br>Bicycle: handlebars, wheels, gears, and a seat.<br>What can you create from these individual parts? One option is to make a snowmobile by combining the handlebars and seat from the bike, the metal treads from the tank, and the motor and skis from the boat.</p><p>This is the process of first principles thinking in a nutshell. It is a cycle of breaking a situation down into the core pieces and then putting them all back together in a more effective way. Deconstruct then reconstruct.</font></p><h1 id="How-First-Principles-Drive-Innovation"><a href="#How-First-Principles-Drive-Innovation" class="headerlink" title="How First Principles Drive Innovation"></a>How First Principles Drive Innovation</h1><p><font face=Georgia size=5>The snowmobile example also highlights another hallmark of first principles thinking, which is the combination of ideas from seemingly unrelated fields. A tank and a bicycle appear to have nothing in common, but pieces of a tank and a bicycle can be combined to develop innovations like a snowmobile.</p><p>Many of the most groundbreaking ideas in history have been a result of boiling things down to the first principles and then substituting a more effective solution for one of the key parts.</p><p>For instance, Johannes Gutenberg combined the technology of a screw press—a device used for making wine—with movable type, paper, and ink to create the printing press. Movable type had been used for centuries, but Gutenberg was the first person to consider the constituent parts of the process and adapt technology from an entirely different field to make printing far more efficient. The result was a world-changing innovation and the widespread distribution of information for the first time in history.</p><p>The best solution is not where everyone is already looking.</p><p>First principles thinking helps you to cobble together information from different disciplines to create new ideas and innovations. You start by getting to the facts. Once you have a foundation of facts, you can make a plan to improve each little piece. This process naturally leads to exploring widely for better substitutes.</font></p><h1 id="The-Challenge-of-Reasoning-From-First-Principles"><a href="#The-Challenge-of-Reasoning-From-First-Principles" class="headerlink" title="The Challenge of Reasoning From First Principles"></a>The Challenge of Reasoning From First Principles</h1><p><font face=Georgia size=5>First principles thinking can be easy to describe, but quite difficult to practice. One of the primary obstacles to first principles thinking is our tendency to optimize form rather than function. The story of the suitcase provides a perfect example.</p><p>In ancient Rome, soldiers used leather messenger bags and satchels to carry food while riding across the countryside. At the same time, the Romans had many vehicles with wheels like chariots, carriages, and wagons. And yet, for thousands of years, nobody thought to combine the bag and the wheel. The first rolling suitcase wasn’t invented until 1970 when Bernard Sadow was hauling his luggage through an airport and saw a worker rolling a heavy machine on a wheeled skid.</p><p>Throughout the 1800s and 1900s, leather bags were specialized for particular uses—backpacks for school, rucksacks for hiking, suitcases for travel. Zippers were added to bags in 1938. Nylon backpacks were first sold in 1967. Despite these improvements, the form of the bag remained largely the same. Innovators spent all of their time making slight iterations on the same theme.</p><p>What looks like innovation is often an iteration of previous forms rather than an improvement of the core function. While everyone else was focused on how to build a better bag (form), Sadow considered how to store and move things more efficiently (function).</font></p><h1 id="How-to-Think-for-Yourself"><a href="#How-to-Think-for-Yourself" class="headerlink" title="How to Think for Yourself"></a>How to Think for Yourself</h1><p><font face=Georgia size=5>The human tendency for imitation is a common roadblock to first principles thinking. When most people envision the future, they project the current form forward rather than projecting the function forward and abandoning the form.</p><p>For instance, when criticizing technological progress some people ask, “Where are the flying cars?”</p><p>Here’s the thing: We have flying cars. They’re called airplanes. People who ask this question are so focused on form (a flying object that looks like a car) that they overlook the function (transportation by flight). This is what Elon Musk is referring to when he says that people often “live life by analogy.”</p><p>Be wary of the ideas you inherit. Old conventions and previous forms are often accepted without question and, once accepted, they set a boundary around creativity.</p><p>This difference is one of the key distinctions between continuous improvement and first principles thinking. Continuous improvement tends to occur within the boundary set by the original vision. By comparison, first principles thinking requires you to abandon your allegiance to previous forms and put the function front and center. What are you trying to accomplish? What is the functional outcome you are looking to achieve?</p><p>Optimize the function. Ignore the form. This is how you learn to think for yourself.</font></p><h1 id="The-Power-of-First-Principles"><a href="#The-Power-of-First-Principles" class="headerlink" title="The Power of First Principles"></a>The Power of First Principles</h1><p><font face=Georgia size=5>Ironically, perhaps the best way to develop cutting-edge ideas is to start by breaking things down to the fundamentals. Even if you aren’t trying to develop innovative ideas, understanding the first principles of your field is a smart use of your time. Without a firm grasp of the basics, there is little chance of mastering the details that make the difference at elite levels of competition.</p><p>Every innovation, including the most groundbreaking ones, requires a long period of iteration and improvement. The company at the beginning of this article, SpaceX, ran many simulations, made thousands of adjustments, and required multiple trials before they figured out how to build an affordable and reusable rocket.</p><p>First principles thinking does not remove the need for continuous improvement, but it does alter the direction of improvement. Without reasoning by first principles, you spend your time making small improvements to a bicycle rather than a snowmobile. First principles thinking sets you on a different trajectory.</p><p>If you want to enhance an existing process or belief, continuous improvement is a great option. If you want to learn how to think for yourself, reasoning from first principles is one of the best ways to do it.</font></p>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>11月3日-11月14日文章分享</title>
      <link href="/2021/11/15/11%E6%9C%883%E6%97%A5-11%E6%9C%8814%E6%97%A5%E6%96%87%E7%AB%A0%E6%8E%A8%E8%8D%90/"/>
      <url>/2021/11/15/11%E6%9C%883%E6%97%A5-11%E6%9C%8814%E6%97%A5%E6%96%87%E7%AB%A0%E6%8E%A8%E8%8D%90/</url>
      
        <content type="html"><![CDATA[<p>@[TOC]</p><h1 id="1-教育"><a href="#1-教育" class="headerlink" title="1.教育"></a>1.教育</h1><h2 id="1-1-Albert-Einstein"><a href="#1-1-Albert-Einstein" class="headerlink" title="1.1 Albert Einstein"></a>1.1 Albert Einstein</h2><p><a href="https://new.qq.com/omn/20210202/20210202A031QZ00.html">爱因斯坦：教育的首要目标是什么？—腾讯新闻</a></p><h2 id="1-2-论学术与学术标准—李伯重"><a href="#1-2-论学术与学术标准—李伯重" class="headerlink" title="1.2 论学术与学术标准—李伯重"></a>1.2 论学术与学术标准—李伯重</h2><p><a href="https://www.cnki.net/KCMS/detail/detail.aspx?filename=SKLU200503001&dbname=cjfdtotal&dbcode=CJFD&v=MDk5NjFpYkhlN0c0SHRUTXJJOUZaWVI2RGc4L3poWVU3enNPVDNpUXJSY3pGckNVUjd1ZVp1ZHFGQ3JsVjc3QU4=">论学术与学术标准—李伯重</a></p><h2 id="1-3-康毅滨"><a href="#1-3-康毅滨" class="headerlink" title="1.3 康毅滨"></a>1.3 康毅滨</h2><p><a href="https://mp.weixin.qq.com/s/tYg3wwxJxPNrK7g4L3DRWg">《星期日新闻晨报》康毅滨访谈</a></p><h2 id="1-4-亲子教育"><a href="#1-4-亲子教育" class="headerlink" title="1.4 亲子教育"></a>1.4 亲子教育</h2><p><a href="https://mp.weixin.qq.com/s/EC2RzHZ_d4nR85Vs7x-6UA">100幅心理漫画告诉我们：教育可以很简单</a></p><h1 id="2-人物"><a href="#2-人物" class="headerlink" title="2.人物"></a>2.人物</h1><h2 id="2-1-清华大学本科生特等奖学金答辩"><a href="#2-1-清华大学本科生特等奖学金答辩" class="headerlink" title="2.1 清华大学本科生特等奖学金答辩"></a>2.1 清华大学本科生特等奖学金答辩</h2><p><a href="https://www.tsinghua.edu.cn/info/1181/88823.htm">2021年清华大学本科生特等奖学金答辩会举行</a></p><h2 id="2-2-John-von-Neumann"><a href="#2-2-John-von-Neumann" class="headerlink" title="2.2 John von Neumann"></a>2.2 John von Neumann</h2><p><a href="https://www.cantorsparadise.com/the-unparalleled-genius-of-john-von-neumann-791bb9f42a2d">The Unparalleled Genius of John von Neumann—Jørgen Veisdal</a></p><h2 id="2-3-华科团队EDA全球冠军"><a href="#2-3-华科团队EDA全球冠军" class="headerlink" title="2.3 华科团队EDA全球冠军"></a>2.3 华科团队EDA全球冠军</h2><p><a href="http://iccad-contest.org/2021/Problems.html">CAD Contest</a></p><p><a href="http://iccad-contest.org/2021/ProblemB-cada0136.mp4">华科团队EDA全球冠军解决方案mp4</a></p><h2 id="2-4-Jure-Leskovec"><a href="#2-4-Jure-Leskovec" class="headerlink" title="2.4 Jure Leskovec"></a>2.4 Jure Leskovec</h2><p>研究方向:<br>applied machine learning for large interconnected systems focusing on modeling complex, richly-labeled relational structures, graphs, and networks for systems at all scales, from interactions of proteins in a cell to interactions between humans in a society. Applications include commonsense reasoning, recommender systems, computational social science, and computational biology with an emphasis on drug discovery.</p><p><a href="https://cs.stanford.edu/people/jure/">Jure Leskovec  @  Stanford</a></p><p><a href="https://arxiv.org/abs/1810.00826#">Paper—How Powerful are Graph Neural Networks?</a></p><p><a href="https://static.aminer.cn/misc/pdf/graphsage2-mit-nov19.pdf">Jure Leskovec清华演讲PPT—AMiner</a></p><h2 id="2-5-Yann-LeCun"><a href="#2-5-Yann-LeCun" class="headerlink" title="2.5 Yann LeCun"></a>2.5 Yann LeCun</h2><p><a href="https://www.yicai.com/news/5272525.html">法国极客Yann LeCun：掌舵Facebook人工智能 | 完美人物志</a></p><p><a href="https://www.leiphone.com/category/ai/62TcDCKFomfCEWnQ.html">Yann LeCun专访：我不觉得自己有天分，但是我一直往聪明人堆里钻</a></p><h2 id="2-6-何恺明"><a href="#2-6-何恺明" class="headerlink" title="2.6 何恺明"></a>2.6 何恺明</h2><p><a href="http://kaiminghe.com/">Kaiming He</a></p><p><a href="https://arxiv.org/abs/2111.06377">Masked Autoencoders Are Scalable Vision Learners</a></p><p><a href="https://ai.facebook.com/">FAIR</a></p><h2 id="2-7-Gabor-Fodor"><a href="#2-7-Gabor-Fodor" class="headerlink" title="2.7 Gábor Fodor"></a>2.7 Gábor Fodor</h2><p><a href="https://www.kaggle.com/gaborfodor">Kaggle—beluga</a></p><p><a href="https://mp.weixin.qq.com/s/3v1LaqwpcfSnFop4hy_jRw">Kaggle GM Gábor：成绩排名说明一切</a></p><h2 id="2-8-柳叶熙"><a href="#2-8-柳叶熙" class="headerlink" title="2.8 柳叶熙"></a>2.8 柳叶熙</h2><p><a href="https://space.bilibili.com/535128436?from=search&seid=14166111367059293020&spm_id_from=333.337.0.0">创壹科技—柳夜熙—bilibili</a></p><h1 id="3-前沿"><a href="#3-前沿" class="headerlink" title="3.前沿"></a>3.前沿</h1><h2 id="3-1-戴琼海-发现、理解与创造"><a href="#3-1-戴琼海-发现、理解与创造" class="headerlink" title="3.1 戴琼海:发现、理解与创造"></a>3.1 戴琼海:发现、理解与创造</h2><p><a href="https://www.cxyinfo.com/cms/show-7366.html">戴琼海院士谈人工智能未来：发现、理解与创造</a></p><h2 id="3-2-CoRL-2021-Awards"><a href="#3-2-CoRL-2021-Awards" class="headerlink" title="3.2 CoRL 2021 Awards"></a>3.2 CoRL 2021 Awards</h2><p><a href="https://www.robot-learning.org/program/awards_2021">CoRL 2021 Awards</a></p><h2 id="3-3-IJCLR-Zhi-Hua-Zhou-“From-Pure-Learning-to-Learning-amp-Reasoning”"><a href="#3-3-IJCLR-Zhi-Hua-Zhou-“From-Pure-Learning-to-Learning-amp-Reasoning”" class="headerlink" title="3.3 IJCLR  Zhi-Hua Zhou: “From Pure Learning to Learning &amp; Reasoning”"></a>3.3 IJCLR  Zhi-Hua Zhou: “From Pure Learning to Learning &amp; Reasoning”</h2><p><a href="http://lr2020.iit.demokritos.gr/">IJCLR</a></p><p><a href="https://www.youtube.com/playlist?list=PL18_rB75vx1PkjXnkX1jiqNeNnVCbNGIh">YouTube—1st International Joint Conference on Learning &amp; Reasonin</a></p><p><a href="https://www.youtube.com/watch?v=LAvRDCcXCMc&list=PL18_rB75vx1PkjXnkX1jiqNeNnVCbNGIh&index=3&ab_channel=Inst.Informatics&Telecomms,NCSRDemokritos">YouTube—IJCLR 2021 Keynote Talk by Zhi-Hua Zhou: “From Pure Learning to Learning &amp; Reasoning”</a></p><h2 id="3-4-UC伯克利—每个神经网络，都是一个高维向量"><a href="#3-4-UC伯克利—每个神经网络，都是一个高维向量" class="headerlink" title="3.4 UC伯克利—每个神经网络，都是一个高维向量"></a>3.4 UC伯克利—每个神经网络，都是一个高维向量</h2><p><a href="https://mp.weixin.qq.com/s/HuM5lHEZYdmZAk8H6r5IRQ">UC伯克利发现「没有免费午餐定理」加强版：每个神经网络，都是一个高维向量—图灵人工智能</a></p><p><a href="https://arxiv.org/abs/2110.03922">Neural Tangent Kernel Eigenvalues Accurately Predict Generalization</a></p><h2 id="3-5-2021深度学习方向—知乎"><a href="#3-5-2021深度学习方向—知乎" class="headerlink" title="3.5 2021深度学习方向—知乎"></a>3.5 2021深度学习方向—知乎</h2><p><a href="https://www.zhihu.com/question/460500204">2021年深度学习哪些方向比较新颖，处于上升期或者朝阳阶段，没那么饱和，比较有研究潜力？—陀飞轮 、Zhifeng 、谢凌曦</a></p><h2 id="3-6-字节跳动—视频抠像工具—RNN"><a href="#3-6-字节跳动—视频抠像工具—RNN" class="headerlink" title="3.6 字节跳动—视频抠像工具—RNN"></a>3.6 字节跳动—视频抠像工具—RNN</h2><p><a href="https://arxiv.org/abs/2108.11515">Robust High-Resolution Video Matting with Temporal Guidance</a></p><p><a href="https://github.com/PeterL1n/RobustVideoMatting">Github源码</a></p><p><a href="https://openbayes.com/console/open-tutorials/containers/oqv42tbd8ko">openbayes</a></p><h2 id="3-7-飞桨图像识别套件PaddleClas"><a href="#3-7-飞桨图像识别套件PaddleClas" class="headerlink" title="3.7 飞桨图像识别套件PaddleClas"></a>3.7 飞桨图像识别套件PaddleClas</h2><p><a href="https://github.com/PaddlePaddle/PaddleClas">PaddleClas—GitHub</a></p><h2 id="3-8-Small-Data’s-Big-AI-Potential"><a href="#3-8-Small-Data’s-Big-AI-Potential" class="headerlink" title="3.8 Small Data’s Big AI Potential"></a>3.8 Small Data’s Big AI Potential</h2><p><a href="https://cset.georgetown.edu/publication/small-datas-big-ai-potential/">Small Data’s Big AI Potential</a></p><p><a href="https://cset.georgetown.edu/">CSTE</a></p><p><a href="https://mp.weixin.qq.com/s/DuEk7II2Th7s9Uyr-bx7WQ">美国智库最新报告：长期被忽略的小数据人工智能潜力不可估量—大数据文摘</a></p><h2 id="3-9-Bilingualism-Comes-Naturally-to-Our-Brains"><a href="#3-9-Bilingualism-Comes-Naturally-to-Our-Brains" class="headerlink" title="3.9 Bilingualism Comes Naturally to Our Brains"></a>3.9 Bilingualism Comes Naturally to Our Brains</h2><p><a href="https://www.eneuro.org/content/8/6/ENEURO.0084-21.2021#sec-10">Paper—Composition within and between Languages in the Bilingual Mind: MEG Evidence from Korean/English Bilinguals</a></p><p><a href="https://www.nyu.edu/about/news-publications/news/2021/november/bilingualism-comes-naturally-to-our-brains.html">NYU—Bilingualism Comes Naturally to Our Brains</a></p><h2 id="3-10-内在触感-强化学习-机械手"><a href="#3-10-内在触感-强化学习-机械手" class="headerlink" title="3.10 内在触感  强化学习  机械手"></a>3.10 内在触感  强化学习  机械手</h2><p><a href="https://arxiv.org/abs/2109.12720">Paper—On the Feasibility of Learning Finger-gaiting In-hand Manipulation with Intrinsic Sensing</a></p><h2 id="3-11-大脑学习算法模型模拟反向传播过程"><a href="#3-11-大脑学习算法模型模拟反向传播过程" class="headerlink" title="3.11 大脑学习算法模型模拟反向传播过程"></a>3.11 大脑学习算法模型模拟反向传播过程</h2><p><a href="https://mp.weixin.qq.com/s/RxZhzYDCuAkxfeOa6v7ySA">大脑学习算法模型模拟反向传播过程</a></p><p><a href="https://www.nature.com/articles/s41593-021-00857-x">Paper—Burst-dependent synaptic plasticity can coordinate learning in hierarchical circuits</a></p><p><a href="https://www.quantamagazine.org/brain-bursts-can-mimic-famous-ai-learning-strategy-20211018/">Neuron Bursts Can Mimic Famous AI Learning Strategy</a></p><h1 id="4-学习资源"><a href="#4-学习资源" class="headerlink" title="4.学习资源"></a>4.学习资源</h1><h2 id="4-1-《Statistical-Thinking-for-the-21st-Century》—Stanford-University"><a href="#4-1-《Statistical-Thinking-for-the-21st-Century》—Stanford-University" class="headerlink" title="4.1 《Statistical Thinking for the 21st Century》—Stanford University"></a>4.1 《Statistical Thinking for the 21st Century》—Stanford University</h2><p><a href="https://statsthinking21.github.io/statsthinking21-core-site/">《Statistical Thinking for the 21st Century》</a></p><p><a href="https://github.com/statsthinking21/statsthinking21-core">statsthinking21—Github</a></p><h2 id="4-2-C语言入门笔记"><a href="#4-2-C语言入门笔记" class="headerlink" title="4.2 C语言入门笔记"></a>4.2 C语言入门笔记</h2><p><a href="https://mp.weixin.qq.com/s/-w5lbR4awV-JQQtTGWjylA">C语言最全入门笔记—图灵人工智能</a></p><h2 id="4-3-简单的机器学习模型线性回归"><a href="#4-3-简单的机器学习模型线性回归" class="headerlink" title="4.3 简单的机器学习模型线性回归"></a>4.3 简单的机器学习模型线性回归</h2><p><a href="https://mp.weixin.qq.com/s/n7gjNYEMUFzJuCxuZ47zgQ">初学者指南：使用 Numpy、Keras 和 PyTorch 实现最简单的机器学习模型线性回归—数据派THU</a></p><p><a href="https://github.com/Motamensalih/Simple-Linear-Regression">Simple-Linear-Regression—Github</a></p><h2 id="4-4-17个机器学习的常用算法！"><a href="#4-4-17个机器学习的常用算法！" class="headerlink" title="4.4 17个机器学习的常用算法！"></a>4.4 17个机器学习的常用算法！</h2><p><a href="https://mp.weixin.qq.com/s/PTdArpfF2n9JtgJbmS1G6A">17个机器学习的常用算法！—图灵人工智能</a></p><h2 id="4-5-鱼水说竞赛：竞赛模型选择"><a href="#4-5-鱼水说竞赛：竞赛模型选择" class="headerlink" title="4.5 鱼水说竞赛：竞赛模型选择"></a>4.5 鱼水说竞赛：竞赛模型选择</h2><p><a href="https://mp.weixin.qq.com/s/E-g7faR0AUGstdNMzVnlkw">鱼水说竞赛：竞赛模型选择</a></p><h2 id="4-6-计算机早期历史-Early-Computing"><a href="#4-6-计算机早期历史-Early-Computing" class="headerlink" title="4.6 计算机早期历史-Early Computing"></a>4.6 计算机早期历史-Early Computing</h2><p><a href="https://www.youtube.com/watch?v=WqrNphu6HaU&ab_channel=%E8%B8%8F%E9%9B%AA%E6%97%A0%E7%97%95">计算机早期历史-Early Computing–YouTube</a></p><h2 id="4-7-图神经网络科普"><a href="#4-7-图神经网络科普" class="headerlink" title="4.7 图神经网络科普"></a>4.7 图神经网络科普</h2><p><a href="https://mp.weixin.qq.com/s/nIKHmgTJQU3pyQzaxmuVhw">图神经网络科普</a></p><p><a href="https://distill.pub/2021/gnn-intro/">A Gentle Introduction to Graph Neural Networks</a></p><p><a href="https://distill.pub/2021/understanding-gnns/">Understanding Convolutions on Graphs</a></p><h2 id="4-8-《Mathematical-Foundations-for-Data-Analysis》"><a href="#4-8-《Mathematical-Foundations-for-Data-Analysis》" class="headerlink" title="4.8 《Mathematical Foundations for Data Analysis》"></a>4.8 《Mathematical Foundations for Data Analysis》</h2><p><a href="https://mathfordata.github.io/">《Mathematical Foundations for Data Analysis》—Jeff M. Phillips</a></p><h1 id="5-职场"><a href="#5-职场" class="headerlink" title="5.职场"></a>5.职场</h1><h2 id="5-1-博士-入职-三四流高校-参考意见—知乎"><a href="#5-1-博士-入职-三四流高校-参考意见—知乎" class="headerlink" title="5.1 博士  入职  三四流高校  参考意见—知乎"></a>5.1 博士  入职  三四流高校  参考意见—知乎</h2><p><a href="https://mp.weixin.qq.com/s/Nz_GpKHNQ-1sqFp3vRMt2w">博士  入职  三四线高校  参考意见—图灵人工智能</a></p><h1 id="6-机构、网站"><a href="#6-机构、网站" class="headerlink" title="6.机构、网站"></a>6.机构、网站</h1><h2 id="6-1-北京智源人工智能研究院"><a href="#6-1-北京智源人工智能研究院" class="headerlink" title="6.1 北京智源人工智能研究院"></a>6.1 北京智源人工智能研究院</h2><p><a href="https://www.baai.ac.cn/">北京智源人工智能研究院</a></p><h2 id="6-2-FAIR"><a href="#6-2-FAIR" class="headerlink" title="6.2 FAIR"></a>6.2 FAIR</h2><p><a href="https://ai.facebook.com/">FAIR</a></p><h2 id="6-3-CZ-Biohub"><a href="#6-3-CZ-Biohub" class="headerlink" title="6.3 CZ Biohub"></a>6.3 CZ Biohub</h2><p><a href="https://www.czbiohub.org/">CZ Biohub</a></p><h1 id="7-数据集"><a href="#7-数据集" class="headerlink" title="7.数据集"></a>7.数据集</h1><h2 id="7-1-MedMNIST"><a href="#7-1-MedMNIST" class="headerlink" title="7.1 MedMNIST"></a>7.1 MedMNIST</h2><p>MedMNIST:A Large-Scale Lightweight Benchmark for 2D and 3D Biomedical Image Classification</p><p><a href="https://medmnist.com/">MedMNIST v2</a></p><p><a href="https://github.com/MedMNIST/MedMNIST">MedMNIST—GitHub</a></p><p><a href="https://arxiv.org/pdf/2110.14795.pdf">MedMNIST v2论文</a></p>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>EDG我们是冠军！</title>
      <link href="/2021/11/07/EDG%E6%88%91%E4%BB%AC%E6%98%AF%E5%86%A0%E5%86%9B/"/>
      <url>/2021/11/07/EDG%E6%88%91%E4%BB%AC%E6%98%AF%E5%86%A0%E5%86%9B/</url>
      
        <content type="html"><![CDATA[<p>EDG！ 我们是冠军！<br>Make/Break!<br>不破不立！</p>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>11月3日文章分享</title>
      <link href="/2021/11/03/11%E6%9C%883%E6%97%A5%E6%96%87%E7%AB%A0%E5%88%86%E4%BA%AB/"/>
      <url>/2021/11/03/11%E6%9C%883%E6%97%A5%E6%96%87%E7%AB%A0%E5%88%86%E4%BA%AB/</url>
      
        <content type="html"><![CDATA[<h1 id="1-演讲"><a href="#1-演讲" class="headerlink" title="1.演讲"></a>1.演讲</h1><h2 id="1-1-John-Edward-Hopcroft-《开放科学：科学传播与人才培养》"><a href="#1-1-John-Edward-Hopcroft-《开放科学：科学传播与人才培养》" class="headerlink" title="1.1 John Edward Hopcroft 《开放科学：科学传播与人才培养》"></a>1.1 John Edward Hopcroft 《开放科学：科学传播与人才培养》</h2><p><a href="https://new.qq.com/omn/20211102/20211102A02F6400.html">图灵奖得主：中国应该重视本科教育质量，而不是研究经费和论文数量-腾讯网</a></p><h1 id="2-人物"><a href="#2-人物" class="headerlink" title="2.人物"></a>2.人物</h1><h2 id="2-1-John-Edward-Hopcroft："><a href="#2-1-John-Edward-Hopcroft：" class="headerlink" title="2.1 John Edward Hopcroft："></a>2.1 John Edward Hopcroft：</h2><p><a href="https://news.sjtu.edu.cn/mtjj/20180405/67002.html">图灵奖获得者约翰·霍普克罗夫特：中国高校必须教会学生提问—上海交通大学新闻学术网</a></p><p><a href="http://zqb.cyol.com/html/2012-02/09/nw.D110000zgqnb_20120209_3-03.htm">中国高校必须教会学生提问—中国青年报</a></p><h2 id="2-2-吴天齐"><a href="#2-2-吴天齐" class="headerlink" title="2.2 吴天齐"></a>2.2 吴天齐</h2><p><a href="https://mp.weixin.qq.com/s/TzkUvWwksCjQcvmRYh8Odw">吴齐天的科研思考—DataWhale</a></p><p><a href="https://thinklab.sjtu.edu.cn/">SJTU-ThinkLab官网</a></p><h2 id="2-3-Johnson-Kuan"><a href="#2-3-Johnson-Kuan" class="headerlink" title="2.3 Johnson Kuan"></a>2.3 Johnson Kuan</h2><p><a href="https://towardsdatascience.com/how-i-won-andrew-ngs-very-first-data-centric-ai-competition-e02001268bda">Johnson Kuan：How I Won Andrew Ng’s First Data-Centric AI Competition</a></p><h2 id="2-4-陶中恺"><a href="#2-4-陶中恺" class="headerlink" title="2.4 陶中恺"></a>2.4 陶中恺</h2><p><a href="https://mp.weixin.qq.com/s/5245h0CMA45h8ONNflzb6Q">阿里数学竞赛最年轻金奖得主陶中恺：“学数学还是要自信”</a></p><h1 id="3-新闻"><a href="#3-新闻" class="headerlink" title="3.新闻"></a>3.新闻</h1><h2 id="3-1-VMware-与戴尔正式“分手”"><a href="#3-1-VMware-与戴尔正式“分手”" class="headerlink" title="3.1 VMware 与戴尔正式“分手”"></a>3.1 VMware 与戴尔正式“分手”</h2><p><a href="https://blog.csdn.net/sinat_14921509/article/details/121097972?spm=1000.2115.3001.5927">VMware 与戴尔正式“分手”—苏小宓的CSDN</a></p><p><a href="https://news.vmware.com/leadership/ceo-raghu-raghuram-spin-off-complete">The Start of a New Era for VMware</a></p><h2 id="3-2-ReSkin"><a href="#3-2-ReSkin" class="headerlink" title="3.2 ReSkin"></a>3.2 ReSkin</h2><p><a href="https://ai.facebook.com/blog/reskin-a-versatile-replaceable-low-cost-skin-for-ai-research-on-tactile-perception/">ReSkin: a versatile, replaceable, low-cost skin for AI research on tactile perception</a></p><h1 id="4-实用网站"><a href="#4-实用网站" class="headerlink" title="4.实用网站"></a>4.实用网站</h1><h2 id="4-1-GitHub最大的开源算法库—The-Algorithms"><a href="#4-1-GitHub最大的开源算法库—The-Algorithms" class="headerlink" title="4.1 GitHub最大的开源算法库—The Algorithms"></a>4.1 GitHub最大的开源算法库—The Algorithms</h2><p><a href="https://the-algorithms.com/">The Algorithms</a></p><p><a href="https://github.com/TheAlgorithms">The Algorithms’s GitHub</a></p>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>决赛见！</title>
      <link href="/2021/11/01/%E5%86%B3%E8%B5%9B%E8%A7%81/"/>
      <url>/2021/11/01/%E5%86%B3%E8%B5%9B%E8%A7%81/</url>
      
        <content type="html"><![CDATA[]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>【Colab】基本操作【LeNet】【MNIST】训练测试</title>
      <link href="/2021/10/31/Colab+LeNet+MNIST/"/>
      <url>/2021/10/31/Colab+LeNet+MNIST/</url>
      
        <content type="html"><![CDATA[<p><a href="https://colab.research.google.com/notebooks/welcome.ipynb">Colab 官网初始界面</a></p><h1 id="1-介绍"><a href="#1-介绍" class="headerlink" title="1.介绍"></a>1.介绍</h1><p><a href="https://developer.nvidia.com/zh-cn/cuda-gpus">英伟达官网</a><br>谷歌将原来K80换成了T4<br><img src="https://img-blog.csdnimg.cn/7975074e2ece4cdc8be72e96851fd996.png?x-oss-process=image/watermark,type_ZHJvaWRzYW5zZmFsbGJhY2s,shadow_50,text_Q1NETiBAQVhETE1H,size_20,color_FFFFFF,t_70,g_se,x_16"></p><h1 id="2-查看基本配置"><a href="#2-查看基本配置" class="headerlink" title="2.查看基本配置"></a>2.查看基本配置</h1><h3 id="2-1查看pytorch版本"><a href="#2-1查看pytorch版本" class="headerlink" title="2.1查看pytorch版本"></a>2.1查看pytorch版本</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(torch)</span><br></pre></td></tr></table></figure><p><img src="https://img-blog.csdnimg.cn/b08a271c94da4d0293ac0dbc8ff3cc25.png?x-oss-process=image/watermark,type_ZHJvaWRzYW5zZmFsbGJhY2s,shadow_50,text_Q1NETiBAQVhETE1H,size_13,color_FFFFFF,t_70,g_se,x_16"></p><h3 id="2-2查看是否可以使用cuda"><a href="#2-2查看是否可以使用cuda" class="headerlink" title="2.2查看是否可以使用cuda"></a>2.2查看是否可以使用cuda</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">print</span>(torch.cuda.device_count())</span><br></pre></td></tr></table></figure><p><img src="https://img-blog.csdnimg.cn/cce0d37208584800922981b38136b11f.png"><br>修改-&gt;笔记本设置-&gt;GPU<br><img src="https://img-blog.csdnimg.cn/38bcb5ea6f724e1a8a120bd71614efcc.png?x-oss-process=image/watermark,type_ZHJvaWRzYW5zZmFsbGJhY2s,shadow_50,text_Q1NETiBAQVhETE1H,size_19,color_FFFFFF,t_70,g_se,x_16"></p><h3 id="2-3查看显卡配置"><a href="#2-3查看显卡配置" class="headerlink" title="2.3查看显卡配置"></a>2.3查看显卡配置</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">!nvidia-smi</span><br><span class="line">//注意英文感叹号</span><br></pre></td></tr></table></figure><p><img src="https://img-blog.csdnimg.cn/a222ea06ae0c44ce8f34592494dca1c3.png?x-oss-process=image/watermark,type_ZHJvaWRzYW5zZmFsbGJhY2s,shadow_50,text_Q1NETiBAQVhETE1H,size_20,color_FFFFFF,t_70,g_se,x_16"></p><h1 id="3-挂载"><a href="#3-挂载" class="headerlink" title="3.挂载"></a>3.挂载</h1><h3 id="31-挂载谷歌云盘"><a href="#31-挂载谷歌云盘" class="headerlink" title="31.挂载谷歌云盘"></a>31.挂载谷歌云盘</h3><p>Colab的运行原理实际上就是给你分配一台远程的带GPU的主机，所以它的原始路径不是你的谷歌云盘（也就是你的代码文件）所在的路径。所以第一步我们先要把谷歌云盘挂载带到那台远程主机上：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> google.colab <span class="keyword">import</span> drive</span><br><span class="line">drive.mount(<span class="string">&quot;/content/drive&quot;</span>)</span><br></pre></td></tr></table></figure><p>登录谷歌账号并将验证码粘到框中</p><h3 id="3-2更改运行目录"><a href="#3-2更改运行目录" class="headerlink" title="3.2更改运行目录"></a>3.2更改运行目录</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> sys</span><br><span class="line">sys.path.append(<span class="string">&quot;/content/drive/MyDrive/Colab Notebooks/LeNet_MNIST_train_test&quot;</span>)</span><br></pre></td></tr></table></figure><p>下面是我的目录结构<br><img src="https://img-blog.csdnimg.cn/a96be368c2ef4298af8e0fae94d4f61d.png?x-oss-process=image/watermark,type_ZHJvaWRzYW5zZmFsbGJhY2s,shadow_50,text_Q1NETiBAQVhETE1H,size_20,color_FFFFFF,t_70,g_se,x_16"></p><h1 id="4-训练"><a href="#4-训练" class="headerlink" title="4.训练"></a>4.训练</h1><p>【LeNet】+【MNIST】</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torchvision</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> DataLoader</span><br><span class="line"><span class="keyword">from</span> torchvision <span class="keyword">import</span> transforms</span><br><span class="line"><span class="keyword">import</span> time</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">LeNet</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="built_in">super</span>(LeNet, self).__init__()</span><br><span class="line">        self.model = nn.Sequential(</span><br><span class="line">            nn.Conv2d(<span class="number">1</span>, <span class="number">6</span>, kernel_size=<span class="number">5</span>),</span><br><span class="line">            nn.MaxPool2d(kernel_size=<span class="number">2</span>),</span><br><span class="line">            nn.Conv2d(<span class="number">6</span>, <span class="number">16</span>, kernel_size=<span class="number">5</span>),</span><br><span class="line">            nn.MaxPool2d(kernel_size=<span class="number">2</span>),</span><br><span class="line">            nn.Flatten(),</span><br><span class="line">            nn.Linear(<span class="number">16</span>*<span class="number">4</span>*<span class="number">4</span>, <span class="number">120</span>),</span><br><span class="line">            nn.Linear(<span class="number">120</span>, <span class="number">84</span>),</span><br><span class="line">            nn.Linear(<span class="number">84</span>, <span class="number">10</span>)</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x</span>):</span></span><br><span class="line">        output = self.model(x)</span><br><span class="line">        <span class="keyword">return</span> output</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">train_datasets = torchvision.datasets.MNIST(</span><br><span class="line">    root = <span class="string">r&#x27;../data&#x27;</span>,</span><br><span class="line">    download=<span class="literal">True</span>,</span><br><span class="line">    train=<span class="literal">True</span>,</span><br><span class="line">    transform=transforms.ToTensor()</span><br><span class="line">)</span><br><span class="line">train_dataloader = DataLoader(</span><br><span class="line">    dataset=train_datasets,</span><br><span class="line">    batch_size=<span class="number">64</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">test_datasets = torchvision.datasets.MNIST(</span><br><span class="line">    root = <span class="string">r&#x27;../data&#x27;</span>,</span><br><span class="line">    train=<span class="literal">False</span>,</span><br><span class="line">    download=<span class="literal">True</span>,</span><br><span class="line">    transform=transforms.ToTensor()</span><br><span class="line">)</span><br><span class="line">test_dataloader = DataLoader(</span><br><span class="line">    dataset=test_datasets,</span><br><span class="line">    batch_size=<span class="number">64</span></span><br><span class="line">)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">train_datasets_size = <span class="built_in">len</span>(train_datasets)</span><br><span class="line">test_datasets_size = <span class="built_in">len</span>(test_datasets)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;训练集数量为：&#123;&#125;&quot;</span>.<span class="built_in">format</span>(train_datasets_size))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;测试集数量为：&#123;&#125;&quot;</span>.<span class="built_in">format</span>(test_datasets_size))</span><br><span class="line"></span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">runing_mode = <span class="string">&quot;gpu&quot;</span> <span class="comment"># cpu,gpu, gpus</span></span><br><span class="line"><span class="keyword">if</span> runing_mode == <span class="string">&quot;gpu&quot;</span> <span class="keyword">and</span> torch.cuda.is_available():</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;use cuda&quot;</span>)</span><br><span class="line">    device = torch.device(<span class="string">&quot;cuda&quot;</span>)</span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;use cpu&quot;</span>)</span><br><span class="line">    device = torch.device(<span class="string">&quot;cpu&quot;</span>)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">model = LeNet()</span><br><span class="line">model.to(device)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">loss_fn = nn.CrossEntropyLoss()</span><br><span class="line">loss_fn.to(device)</span><br><span class="line">learning_rate = <span class="number">1e-2</span></span><br><span class="line">optim = torch.optim.SGD(model.parameters(), lr=learning_rate)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line">epoch = <span class="number">10</span></span><br><span class="line">train_step, test_step = <span class="number">0</span>, <span class="number">0</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(epoch):</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;~~~~~~~~~~~~第&#123;&#125;轮训练开始~~~~~~~~~~~&quot;</span>.<span class="built_in">format</span>(i+<span class="number">1</span>))</span><br><span class="line">    start = time.time()</span><br><span class="line">    model.train()</span><br><span class="line">    <span class="keyword">for</span> data <span class="keyword">in</span> train_dataloader:</span><br><span class="line">        imgs, targets = data</span><br><span class="line">        imgs, targets = imgs.to(device), targets.to(device)</span><br><span class="line">        output = model(imgs)</span><br><span class="line">        loss = loss_fn(output, targets)</span><br><span class="line"></span><br><span class="line">        optim.zero_grad()</span><br><span class="line">        loss.backward()</span><br><span class="line">        optim.step()</span><br><span class="line"></span><br><span class="line">        train_step += <span class="number">1</span></span><br><span class="line">        <span class="keyword">if</span> train_step % <span class="number">200</span> == <span class="number">0</span>:</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">&quot;第&#123;&#125;次训练，loss=&#123;:.3f&#125;&quot;</span>.<span class="built_in">format</span>(train_step, loss.item()))</span><br><span class="line">    <span class="comment">#</span></span><br><span class="line">    model.<span class="built_in">eval</span>()</span><br><span class="line">    <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">        test_loss, true_num = <span class="number">0</span>, <span class="number">0</span></span><br><span class="line">        <span class="keyword">for</span> data <span class="keyword">in</span> test_dataloader:</span><br><span class="line">            imgs, targets = data</span><br><span class="line">            imgs, targets = imgs.to(device), targets.to(device)</span><br><span class="line">            output = model(imgs)</span><br><span class="line">            test_loss += loss_fn(output, targets)</span><br><span class="line">            true_num += (output.argmax(<span class="number">1</span>) == targets).<span class="built_in">sum</span>()</span><br><span class="line">    end = time.time()</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;第&#123;&#125;轮测试集上的loss:&#123;:.3f&#125;, 正确率为:&#123;:.3f&#125;%,耗时:&#123;:.3f&#125;&quot;</span>.<span class="built_in">format</span>(test_step+<span class="number">1</span>, test_loss.item(), <span class="number">100</span> * true_num / test_datasets_size, end-start))</span><br><span class="line">    test_step += <span class="number">1</span></span><br></pre></td></tr></table></figure><p>运行结果：<br>1.CPU<br><img src="https://img-blog.csdnimg.cn/14edcf10c0d446a189f4a6fc44fba1ce.png"><br>2.GPU<br><img src="https://img-blog.csdnimg.cn/f396f9c673a24d719516dd5abc055ba1.png"></p><h1 id="5-Reference"><a href="#5-Reference" class="headerlink" title="5.Reference"></a>5.Reference</h1><p><a href="https://blog.csdn.net/u010881576/article/details/120919330">《Colab使用训练指南》 坚强的羊脂球</a></p>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>10月文章分享</title>
      <link href="/2021/10/30/10%E6%9C%88%E6%96%87%E7%AB%A0%E5%88%86%E4%BA%AB/"/>
      <url>/2021/10/30/10%E6%9C%88%E6%96%87%E7%AB%A0%E5%88%86%E4%BA%AB/</url>
      
        <content type="html"><![CDATA[<h1 id="1-演讲"><a href="#1-演讲" class="headerlink" title="1.演讲"></a>1.演讲</h1><h2 id="1-1Steven-Chu：Life-is-too-short-to-go-through-it-without-caring-deeply-about-something"><a href="#1-1Steven-Chu：Life-is-too-short-to-go-through-it-without-caring-deeply-about-something" class="headerlink" title="1.1Steven Chu：Life is too short to go through it without caring deeply about something."></a>1.1Steven Chu：Life is too short to go through it without caring deeply about something.</h2><p><a href="https://news.harvard.edu/gazette/story/2009/06/u-s-energy-secretary-steven-chus-address-at-harvards-afternoon-exercises/">Stenven Chu in_Harvard Commencement 2009</a></p><p><a href="https://ruanyifeng.com/blog/2009/06/remarks_of_stenven_chu_in_harvard_commencement_2009.html">ruanyifeng’s blog:Remarks of Stenven Chu in harvard commencement</a></p><h2 id="1-2李彦宏：创新、跨界、开放的新工科人才"><a href="#1-2李彦宏：创新、跨界、开放的新工科人才" class="headerlink" title="1.2李彦宏：创新、跨界、开放的新工科人才"></a>1.2李彦宏：创新、跨界、开放的新工科人才</h2><p><a href="http://www.pku.org.cn/people/xyjy/1350143.htm">1987级校友李彦宏在北大新工科国际论坛上的演讲</a></p><h2 id="1-3丘成桐-中学数学教育"><a href="#1-3丘成桐-中学数学教育" class="headerlink" title="1.3丘成桐 中学数学教育"></a>1.3丘成桐 中学数学教育</h2><p><a href="https://blog.csdn.net/FnqTyr45/article/details/80490984">丘成桐：中国学生基础真的比欧美学生好吗？</a></p><p>ps：找了一些相同内容的文章，标题太刺眼，感谢这位博主的文章~</p><h1 id="2-人物"><a href="#2-人物" class="headerlink" title="2.人物"></a>2.人物</h1><h2 id="2-1-Klaus-Hasselmann：I-did-not-have-a-real-supervisor"><a href="#2-1-Klaus-Hasselmann：I-did-not-have-a-real-supervisor" class="headerlink" title="2.1 Klaus Hasselmann：I did not have a real supervisor"></a>2.1 Klaus Hasselmann：I did not have a real supervisor</h2><p><a href="https://www.aip.org/history-programs/niels-bohr-library/oral-histories/33645">Oral History Interviews about Klaus Hasselmann on February 15, 2006</a></p><h2 id="2-2-施一公-如何做一名优秀的博士生？"><a href="#2-2-施一公-如何做一名优秀的博士生？" class="headerlink" title="2.2 施一公 如何做一名优秀的博士生？"></a>2.2 施一公 如何做一名优秀的博士生？</h2><p><a href="http://blog.sciencenet.cn/blog-46212-484416.html">（一）时间的付出</a></p><p><a href="http://blog.sciencenet.cn/blog-46212-486270.html">（二）方法论的转变</a></p><h2 id="2-3-Matt-Might：10-easy-ways-to-fail-a-Ph-D"><a href="#2-3-Matt-Might：10-easy-ways-to-fail-a-Ph-D" class="headerlink" title="2.3 Matt Might：10 easy ways to fail a Ph.D."></a>2.3 Matt Might：10 easy ways to fail a Ph.D.</h2><p><a href="https://matt.might.net/articles/ways-to-fail-a-phd/">Matt Might：ways to fail a Ph.D.</a></p><h2 id="2-4-跟李沐学AI"><a href="#2-4-跟李沐学AI" class="headerlink" title="2.4 跟李沐学AI"></a>2.4 跟李沐学AI</h2><p><a href="https://www.bilibili.com/read/cv13335461/">李沐：用随机梯度下降来优化人生</a></p><p>沐神的b站：<a href="https://space.bilibili.com/1567748478">跟李沐学AI</a></p><h2 id="2-5-LShang001"><a href="#2-5-LShang001" class="headerlink" title="2.5 LShang001"></a>2.5 LShang001</h2><p>LShang001的b站：<a href="https://i0.hdslb.com/bfs/space/cb1c3ef50e22b6096fde67febe863494caefebad.png">LShang001</a></p><h2 id="2-6-稚晖君"><a href="#2-6-稚晖君" class="headerlink" title="2.6 稚晖君"></a>2.6 稚晖君</h2><p>稚晖君的b站：<a href="https://space.bilibili.com/20259914?from=search&seid=1336220716890113133&spm_id_from=333.337.0.0">稚晖君</a></p><p>稚晖君的Github：<a href="https://github.com/peng-zhihui">稚晖</a></p><h2 id="2-7-张焕晨-读博，你真的想好了吗？"><a href="#2-7-张焕晨-读博，你真的想好了吗？" class="headerlink" title="2.7 张焕晨 读博，你真的想好了吗？"></a>2.7 张焕晨 读博，你真的想好了吗？</h2><p><a href="https://zhuanlan.zhihu.com/p/372884253">张焕晨：读博，你真的想好了吗？</a></p><p>ps：根据个人需求有选择地阅读，尽量不要受到他人评论影响~</p><h1 id="3-前沿新闻"><a href="#3-前沿新闻" class="headerlink" title="3.前沿新闻"></a>3.前沿新闻</h1><h2 id="3-1-Ghost-Robotics-CEO-Jiren-Parikh：If-our-robot-had-tracks-on-it-instead-of-legs-nobody-would-be-paying-attention"><a href="#3-1-Ghost-Robotics-CEO-Jiren-Parikh：If-our-robot-had-tracks-on-it-instead-of-legs-nobody-would-be-paying-attention" class="headerlink" title="3.1 Ghost Robotics CEO Jiren Parikh：If our robot had tracks on it instead of legs, nobody would be paying attention."></a>3.1 Ghost Robotics CEO Jiren Parikh：If our robot had tracks on it instead of legs, nobody would be paying attention.</h2><p><a href="https://spectrum.ieee.org/ghost-robotics-armed-military-robots">Interview with Jiren Parikh, CEO of Ghost Robotics by IEEE Spectrum.</a></p><h2 id="3-2中科院脑智卓越中心徐波、蒲慕明联合研究团队近期借助生物网络中发现的介观尺度自组织反向传播机制（Self-backpropagation，SBP）"><a href="#3-2中科院脑智卓越中心徐波、蒲慕明联合研究团队近期借助生物网络中发现的介观尺度自组织反向传播机制（Self-backpropagation，SBP）" class="headerlink" title="3.2中科院脑智卓越中心徐波、蒲慕明联合研究团队近期借助生物网络中发现的介观尺度自组织反向传播机制（Self-backpropagation，SBP）"></a>3.2中科院脑智卓越中心徐波、蒲慕明联合研究团队近期借助生物网络中发现的介观尺度自组织反向传播机制（Self-backpropagation，SBP）</h2><p><a href="https://www.science.org/doi/10.1126/sciadv.abh0146">Self-backpropagation of synaptic modifications elevates the efficiency of spiking and artificial neural networks</a></p>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>这篇博客的诞生</title>
      <link href="/2021/10/30/%E6%88%91%E7%9A%84%E5%8D%9A%E5%AE%A2/"/>
      <url>/2021/10/30/%E6%88%91%E7%9A%84%E5%8D%9A%E5%AE%A2/</url>
      
        <content type="html"><![CDATA[<p><a href="https://blog.csdn.net/weixin_52034760/article/details/121047628">Hexo+Github搭建blog</a></p><p>这里为了不熟悉Markdown语法，挂上官方连接：<br><a href="https://markdown.com.cn/basic-syntax/">Markdown基本语法</a></p><p>02 Apr，2021&lt;<br>1.小白<br>2.不适应Shell<br>3.不了解Markdown及HTML</p><p>30 Oct，2021<br>1.逐渐适应Shell<br>2.Markdown基础语法<br>3.Git原理未了解<br>4.GitHub使用增多，Gist、论文源代码、开源项目（少）<br>5.SSH原理未了解<br>6.URL原理未了解<br>7.Github中的master暂未了解<br>8.默认主题</p>]]></content>
      
      
      
    </entry>
    
    
  
  
</search>
