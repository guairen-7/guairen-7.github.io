<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8">
  
  <title>【李沐B站课程：实战机器学习】【有感触的评论】 | AXDLMG7&#39;s Blog</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="description" content="今天学习沐神的：【更新中】斯坦福2021秋季·实用机器学习【中文】【合集】，看到评论区一些有感触的评论，整理下来学习（未总结，值得看一看）：  1.1一定要博士毕业：呜呜呜沐神真的是我等机器学习学生的福音，改变志向不去微软了我去亚麻 跟李沐学AI：还是找到适合自己的组重要【龇牙】  小奶鸟：老师，这门课的更新时间是会按照斯坦福的更新时间同步上线呢？ 还是老师您自己安排呢？ 跟李沐学AI：我打算是周">
<meta property="og:type" content="article">
<meta property="og:title" content="【李沐B站课程：实战机器学习】【有感触的评论】">
<meta property="og:url" content="https://guairen-7.github.io/2022/08/24/%E3%80%90%E6%9D%8E%E6%B2%90B%E7%AB%99%E8%AF%BE%E7%A8%8B%EF%BC%9A%E5%AE%9E%E6%88%98%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E3%80%91%E3%80%90%E6%9C%89%E6%84%9F%E8%A7%A6%E7%9A%84%E8%AF%84%E8%AE%BA%E3%80%91/index.html">
<meta property="og:site_name" content="AXDLMG7&#39;s Blog">
<meta property="og:description" content="今天学习沐神的：【更新中】斯坦福2021秋季·实用机器学习【中文】【合集】，看到评论区一些有感触的评论，整理下来学习（未总结，值得看一看）：  1.1一定要博士毕业：呜呜呜沐神真的是我等机器学习学生的福音，改变志向不去微软了我去亚麻 跟李沐学AI：还是找到适合自己的组重要【龇牙】  小奶鸟：老师，这门课的更新时间是会按照斯坦福的更新时间同步上线呢？ 还是老师您自己安排呢？ 跟李沐学AI：我打算是周">
<meta property="og:locale" content="zh_CN">
<meta property="article:published_time" content="2022-08-24T14:31:29.794Z">
<meta property="article:modified_time" content="2022-08-24T14:37:24.995Z">
<meta property="article:author" content="AXDLMG7">
<meta property="article:tag" content="Mu Li - 经验">
<meta name="twitter:card" content="summary">
  
  
    <link rel="shortcut icon" href="/favicon.ico">
  
  
<link rel="stylesheet" href="/css/typing.css">

  
<link rel="stylesheet" href="/css/donate.css">

  
  
    
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@3/dist/jquery.fancybox.min.css">

  
<meta name="generator" content="Hexo 5.4.1"></head>

  
    
      <body>
    
  
      <div id="container" class="container">
        <article id="post-【李沐B站课程：实战机器学习】【有感触的评论】" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <header id="header" class="header">
  <nav class="mobile-nav">
    <h1 class="nickname">爱笑的冷面鬼7</h1>
    <ul class="mobile-nav-menu">
      <label for="mobile-menu-toggle"><a id="menu-button">&#9776; Menu</a></label>
      <input type="checkbox" id="mobile-menu-toggle"/>
      <ul class="mobile-nav-link">
        
        <a href="/">Home</a>
        
        <a href="/archives">Archives</a>
        
        <a href="/about">About</a>
        
      </ul>
    </ul>
  </nav>
	
		<nav id="main-nav" class="main-nav nav-left">
	
	
	  <a class="main-nav-link" href="/">Home</a>
	
	  <a class="main-nav-link" href="/archives">Archives</a>
	
	  <a class="main-nav-link" href="/about">About</a>
	
  </nav>
</header>

  <hr/>
  <div class="article-inner">
    

    
      <header class="article-header">
        
  
    <h1 class="p-name article-title" itemprop="headline name">
      【李沐B站课程：实战机器学习】【有感触的评论】
    </h1>
  

      </header>
    
    <div class="e-content article-entry typo" itemprop="articleBody">
      
        <p>今天学习沐神的：<a target="_blank" rel="noopener" href="https://space.bilibili.com/1567748478/channel/collectiondetail?sid=28144">【更新中】斯坦福2021秋季·实用机器学习【中文】【合集】</a>，看到评论区一些有感触的评论，整理下来学习（未总结，值得看一看）：</p>
<hr>
<h1 id="1-1"><a href="#1-1" class="headerlink" title="1.1"></a>1.1</h1><p>一定要博士毕业：<br>呜呜呜沐神真的是我等机器学习学生的福音，改变志向不去微软了我去亚麻</p>
<p>跟李沐学AI：<br>还是找到适合自己的组重要【龇牙】</p>
<hr>
<p>小奶鸟：<br>老师，这门课的更新时间是会按照斯坦福的更新时间同步上线呢？ 还是老师您自己安排呢？</p>
<p>跟李沐学AI：<br>我打算是周一、三、五每天更新一节。 所以跟斯坦福的课不是完全同步。 主要原因是中文版每一节讲得会慢和细一些，所以时长应该是比英文版要长</p>
<hr>
<p>我才两岁吖：<br>想知道这门课有配套教材吗？</p>
<p>跟李沐学AI：<br>暂时没有。 也许以后会写一本</p>
<hr>
<p>不知名划水xk：<br>研一新生，等我学一学期Python和机器学习的基础，一定再回来好好学习这个视频[奋斗]</p>
<p>不知名划水xk<br>回复 @枯木流水残桥 :是吗[思考]主要我不知道要学到什么程度再看这个比较合适</p>
<p>枯木流水残桥<br>研一你花一整个学期真没必要，两个星期差不多</p>
<hr>
<p>东南宫怜：<br>沐哥，我是一个211大学数据科学与大数据技术专业的大二学生，之前听了你的《动手学深度学习》的课后，真的是受益匪浅。之前自己看书没看明白的地方，基本上现在都很清晰明了了。我以后的职业规划就是打算从事数据科学相关的研究，但是通过收集一些网上的信息，诸如像知乎那些，我有几点疑问想请教一下沐哥：<br>（1）像是大数据的一些开发平台，如Hadoop，spark，Kafka那些，是不是很有必要去学习？因为我们学校把这个专业安排在数学学院，所以我们现在基本上都是在学数学系的内容。如果再去学习那些大数据开发的知识，再加上本身自己现在正在自学的机器学习，我担心精力会跟不上，而且学下来效果也不好。<br>（2）如果以成为数据科学家为职业规划的话，那什么样的路线去学习数据科学才是一条科学的路线呢？需要在哪些方面着重下力气打好基础呢？<br>（3）能推荐一些比较好的，反映业界前沿的阅读材料吗？一次问了好多问题，实在是麻烦沐哥了</p>
<p>游泳的老萝卜：<br>我试着给你回复一下吧。大数据领域包括两块，第一是基础设置部分，类似于hadoop如何存数据，spark的有向无环图的原理，以及master slaver的体系结构，在例如其如果其task如何分布执行，kafka的消息队列如何设置等等，这些大概更偏cs体系结构方向, 一个就是数据处理分析，我个人认为这个面比较大，例如统计机器学习，或者深度学习都可以认为是这个领域的一种手段，大概数据数学和cs的交叉学科。  如果你是要做数据开发，那大部分工作可能是在第一个方向，如果你要做机器学习可能第二个方希相关性更大一些。<br>我国因为社会发展的需要设置了很多让人很烧脑的专业，其实对标一下国外无非就是 cs,ee和数学。<br>至于你说的担心精力不够的问题，其实你还有很多时间去学习，现在打好基础吧。 基础是什么呢，我觉得就是基本的数学，cs基础课程和代码能力，如果数学理论不懂，外加python或者java或者scala也不会，不知道什么是深度优先，甚至不知道堆区和栈区的区别，那考虑的再多估计也没啥用。</p>
<p>咸鱼快去读书：<br>如果以成为数据科学家为目标，我觉得你提到的那些都没必要学，倒是要认真学一学线性代数，概率统计，博弈论，数据结构与算法之类的最基础的课程，辅以机器学习等工具。至于你提到的hadoop，spark，kafka，看得出来你对这些词并不了解，因为它们分别是三个不同的东西区别还挺大，学这些大概是为了开发岗就业？如果是这样，好好学数据结构与算法，操作系统，数据库，计算机网络，多刷leetcode，平时自己可以做个网站之类的实践一下<br>大数据不等于数据科学，倒是和分布式关系还挺大的。反正打好基础就行，数学基础，编程基础，还有一些计算机基础课程的基础。你甚至可以什么都不懂，读研以后再了解。哪怕是大厂专门做大数据开发的团队，也更看重校招生的基础牢不牢固，你对大数据这一套有了解只会给你锦上添花</p>
<p>瓶碰盆88：<br>大数据其实和ML是不同领域（尽管通常ML会用到大量的数据来训练模型）建议：学校的基础课要好好学，吃透，把基础打牢。同时抽出时间自学计算机方面的东西。至于说选什么方向，现在才大二，还不能过早下结论，不是不能，是不要过早限定自己，你的学习之路就像ML训练模型，不要认为破坏模型的泛化性能</p>
<hr>
<h1 id="1-2"><a href="#1-2" class="headerlink" title="1.2"></a>1.2</h1><p>肖万英雄不朽：<br>大二人工智能专业咸鱼 想请教一下各位大佬 先学机器学习还是深度学习 后悔死了 考了个普通一本还敢报人工智能 学校课程太水了 求大佬们指点</p>
<p>跟李沐学AI：<br>编程，统计，线性代数是基础。然后机器学习，再深度学习</p>
<p>贫尼无欲无求：<br>我觉得你可以先学数分，线代，概统这三门课，这三门课学完以后机器学习大部分的推导你应该是能看懂的。 然后你可以根据需要，选择直接开始学机器学习(传统的那些东西)，还是继续学一些优化，多元统计，线性回归，随机过程等统计理论的课(取决于你以后要干嘛，搞研究的话先把这些理论的课弄清楚，上班的话直接开始机器学习就好了）。 深度学习(李老师这门课)我觉得可以在你学机器学习的同时开始学，但是李老师视频有点简略，细节不多。 你可以李沐李宏毅两个课一起看。。 至于编程是随时随地都要学的</p>
<hr>
<h1 id="1-3"><a href="#1-3" class="headerlink" title="1.3"></a>1.3</h1><p>_ _ Texas _ _：<br>省流版<br>会bs4本期可以跳过本期<br>AGENDA<br>scrawing 简单介绍<br>一个bs4爬billow的简单example<br>(可以忽略的)scrawing价格的cost (1M 100instances 16.6<br> scraw images (简单code和demo 拼url)<br>legal consideration (版权视频/音乐/图片保存的问题 建议咨询律师和法务bp<br>summary 24:29</p>
<p>跟李沐学AI：<br>不能简单就跳哈，后面好几个小节是基于这个</p>
<hr>
<p>斯蒂芬·周30：<br>李沐老师，这里是会教具体网络数据抓取技术，还是这里只是提及一下</p>
<p>跟李沐学AI：<br>就是提一下，而且这数据集之后会用</p>
<hr>
<p>小黄2号驾驶员：<br>老师这个机器学习系列零基础也可以学习吗？</p>
<p>Bibi大学在读学生：<br>有时间可以先听听，等那天一点听不懂了。 你大概也就知道自己该学啥了。 然后再过来听</p>
<hr>
<h1 id="1-4"><a href="#1-4" class="headerlink" title="1.4"></a>1.4</h1><p>音乐大师汉斯季默：<br>感觉这门课代码量不多呀，后面会有一些跟代码相关的demo吗</p>
<p>跟李沐学AI：<br>下一节是代码。 不过确实代码比之前少，这样可以多讲点算法</p>
<hr>
<h1 id="2-1"><a href="#2-1" class="headerlink" title="2.1"></a>2.1</h1><p>BLUE_Flipped：<br>老师，请问这个视频涉及的数据集以及代码会分享出来嘛？</p>
<p>跟李沐学AI：<br>会在第一次作业中分享出来</p>
<hr>
<p>小鱼儿yoga：<br>兄弟们，想问个事:目前研一，导师让我去学强化学习[辣眼睛]。但是我真零基础[笑哭][笑哭]！想知道需要先学一些机器学习或者深度学习吗？</p>
<p>Jason-cs：<br>直接学Cs294 (Berkeley)就好，遇到不懂的concept去google或者references里学一下就好。</p>
<hr>
<p>PS:在B站学习时可以借鉴用户爱喝水的崩奔的笔记、字幕</p>
<hr>
<p>归属者：<br>沐神，house_sales.zip，有没有下载链接？</p>
<p>跟李沐学AI：<br>会在第一次作业中分享出来<br><a target="_blank" rel="noopener" href="https://c.d2l.ai/stanford-cs329p/assignments.html#assignment-1">Stanford-CS329P-Assignment-1</a></p>
<hr>
<p>孤羽zzZ：<br>有人说说这个和西瓜书之间有什么层级或者递进关系吗，在时间顺序上建议怎么安排. 刚入门，求指导[tv_流泪]</p>
<p>_ _ LJR _ _：<br>西瓜书主要讲理论知识，这门课是面向实际应用的。</p>
<p>-雷乌斯-：<br>西瓜书一遍应该看不太懂的吧，当然很多大佬[总督]当作工具书查，然后自己跑代码，跑到一定程度再看西瓜书感受会很不一样[总督]</p>
<p>跟李沐学AI：<br>我们这里假设同学们有了一定的机器学习基础。</p>
<hr>
<p>PS:常见报错可翻找评论区</p>
<hr>
<h1 id="2-2"><a href="#2-2" class="headerlink" title="2.2"></a>2.2</h1><p>SeanTam2003：<br>牧神，数据清理工具能稍微讲一下或者推荐一些吗？[脱单doge]</p>
<p>跟李沐学AI<br>试试OpenRefine或Trifacta Wrangler？</p>
<hr>
<h1 id="2-3"><a href="#2-3" class="headerlink" title="2.3"></a>2.3</h1><h1 id="2-4"><a href="#2-4" class="headerlink" title="2.4"></a>2.4</h1><p>肖恩塔姆2003：<br>没有理解svm的特征工程和机器学习的特征工程区别在哪里，是机器学习的特征工程需要做的工作更简单了吗？[尴尬]</p>
<p>肖恩塔姆2003：<br>个人理解如下：<br>以前SVM时代，需要将大量注意力放在特征工程上，而后续的程序处理并不占据主要工作；<br>到了机器学习时代则不同，特征工程只占据一小部分，而后续的神经网络构建与处理占到很大一部分工作。<br>是这样理解吗？ [微笑]</p>
<p>浊瑜桑：<br>回复 @SeanTam2003 :SVM是机器学习方法的一种。 时代的区别是指早期的机器学习时代和后来的深度学习时代。 机器学习需要明确且规则的输入，比如10列特征1列label形成的表，但是很多任务是不能直接获取这些特征或者特征太多需要减少，比如图片分类任务，一张图片有数百万个像素点，不可能全部作为特征，所以需要有类似于SIFT等基于先验知识的手工方法提取特征然后用机器学习分类； 深度学习由于有复杂的神经网络所以可以直接从图片中提取到特征，而且可以把特征压缩到很简洁，最后用简单的分类方法比如简单线性分类就可以实现分类了。 换句话说，神经网络取代了手工实现特征工程的过程。</p>
<hr>
<p>王家佳0：<br>请教下沐神和各位小伙伴们，金融方面做违约和营销有可能做预训练和迁移学习吗？</p>
<p>Curtis_l：<br>你在这问还不如去搜文献</p>
<hr>
<h1 id="2-5"><a href="#2-5" class="headerlink" title="2.5"></a>2.5</h1><h1 id="3-1"><a href="#3-1" class="headerlink" title="3.1"></a>3.1</h1><p>投个大两分：<br>沐神，学校的lecture都是两小时，怎么视频就8分钟</p>
<p>跟李沐学AI：<br>一节课会讲几个topic，每个topic一般10-30分钟</p>
<hr>
<p>看门大爷小航航：<br>沐神，我是本科车辆，研究生cs在读快毕业了，目前想进企业做智能车感知。 我感觉没能力解决领域前沿问题，不知道要不要继续读博深造，我看智能驾驶公司一堆博士大佬，慌得一批[总督][总督]</p>
<p>跟李沐学AI：<br>先去试试吧，不然怎么会知道行不行？</p>
<hr>
<p>闭图像：<br>请问沐神可以讲lle、le嘛</p>
<p>跟李沐学AI：<br>lle和le分别是什么？</p>
<p>闭图像：<br>回复 @跟李沐学AI :哇哦沐神回我了</p>
<p>闭图像：<br>回复 @跟李沐学AI :我的老师讲机器学习分为分类、聚类、降维、回归，但是我觉得沐神视频中的分法更好一些</p>
<p>闭图像：<br>回复 @跟李沐学AI :lle和le分别是Locally Linear Embedding和Laplacian Eigenmaps</p>
<p>跟李沐学AI：<br>回复 @闭图像 :哦，这次不会讲这一块了。 已经是15年前技术了，最近似乎没有大的突破，所以就是作为工具用了</p>
<hr>
<p>穿梭迷雾：<br>目标函数和损失有啥区别喝联系呀</p>
<p>葱叶v587：<br>一个是函数，一个是函数值吧</p>
<hr>
<h1 id="3-2"><a href="#3-2" class="headerlink" title="3.2"></a>3.2</h1><h1 id="3-3"><a href="#3-3" class="headerlink" title="3.3"></a>3.3</h1><p>见光死个屁：<br>机器学习里也有很多运筹学的内容呀</p>
<p>见光死个屁：<br>运筹学的学习对以后想从事ai工作有帮住吗？</p>
<p>绅手党：<br>回复 @见光死个屁 :统计学的更有用</p>
<p>王俊二：<br>回复 @见光死个屁 :机器学习里面用到的是优化方面的知识，想转ai的话直接学最优化方法或者凸优化就好，没必要学运筹学。</p>
<hr>
<h1 id="3-4"><a href="#3-4" class="headerlink" title="3.4"></a>3.4</h1><p>合合华2018：<br>我是商科的，老师也会教一些机器学习的算法，但是商科讲得不细，所以这个视频我听起来有点吃力，请问同学们，我应该补充哪些前置课程，有没有推荐的视频</p>
<p>16630789614_bili：<br>我也是商科的，你要学的深的话要不然先学下凸优化？ 或者看白板推导机器学习</p>
<hr>
<h1 id="3-5"><a href="#3-5" class="headerlink" title="3.5"></a>3.5</h1><p>浊瑜桑：<br>有没有大佬… 请教一个问题，为什么要用small batch而不是直接SDG？ 我想的是small batch通过多个epoch的循环实际上做到了类似于迁移学习的效果，可以这么理解吗？</p>
<p>宇哥小玩闹：<br>直接sgd还有什么随机性？ 那就叫gd了</p>
<p>浊瑜桑：<br>回复 @宇哥小玩闹 :大佬你来辣！ 你说的对，没有随机性了。 我其实想问的是为什么需要这个随机性？</p>
<p>爱睡觉的小依：<br>回复 @浊瑜桑 :这应该主要是看你的问题的难度，因为每一个具体的数据所对应的误差函数空间不同，通过一定的随机性能够有效避免很多坑坑洼洼，更容易达到这个模型的最好效果。</p>
<p>浊瑜桑：<br>回复 @爱睡觉的小依 :懂了，所以增加随机性是为了避免数据集的特定排列或者某些数据所带来的局部极值。 那我是不是可以进一步这样理解，实际上也可以对总数据集进行打乱顺序的随机排列来减弱这个问题，但是小批量可以避免特定数据的问题，而且还能够减少学习时间</p>
<p>爱睡觉的小依：<br>回复 @浊瑜桑 :如果从代码的角度来看，没有小批量只是随机打乱实则没有打乱，因为只有更新了网络参数，才算是一次优化，此时才在误差函数空间上移动了，那么打乱也是更新一次，没有打乱也是更新一次，最后两者就是一回事了。</p>
<hr>
<p>霹雳吧啦Wz：<br>沐神，什么时候能讲下自监督算法，期待[呲牙]</p>
<p>东纬1998：<br>我喜欢的up主评论我喜欢的up主[总督]</p>
<p>百叶666：<br>活捉</p>
<p>斯温特：<br>自监督现在最常见的就是完形填空了，文本挖掉一个词进行预测（语言模型）或者图像挖掉一块进行预测</p>
<p>安川美：<br>没想到导师也追沐神[总督][总督]</p>
<p>理科不好的工科生：<br>导师！</p>
<p><a target="_blank" rel="noopener" href="https://space.bilibili.com/18161609">Bilibili@霹雳吧啦Wz</a><br><a target="_blank" rel="noopener" href="https://blog.csdn.net/qq_37541097">CSDN@太阳花的小绿豆</a><br><a target="_blank" rel="noopener" href="https://github.com/WZMIAOMIAO/deep-learning-for-image-processing">Github@WuZhe</a><br>微信公众号：阿喆学习小记</p>
<hr>
<h1 id="3-6"><a href="#3-6" class="headerlink" title="3.6"></a>3.6</h1><p>dninebge：<br>老师讲讲gnn吧[吃瓜]</p>
<p>嘉然今天吃时间晶体：<br>动手学深度学习里面有</p>
<p>斯温特：<br>斯坦福cs224w讲的很好</p>
<p>幼儿园AI学习的小朋友：<br><a target="_blank" rel="noopener" href="https://b23.tv/BV1nu411e7yb">【GNN 图神经网络】直观透彻理解</a>看到一个UP讲的深入浅出，还有动画</p>
<hr>
<h1 id="3-7"><a href="#3-7" class="headerlink" title="3.7"></a>3.7</h1><p>最后一天：<br>能不能斗胆问一下沐神，关于数据结构和算法，刷leetcode，您能提供一些建议吗？[笑哭]</p>
<p>跟李沐学AI：<br>多刷几遍，也没坏处</p>
<hr>
<p>corvusy：<br>沐神，请教一下，RNN类的算法都默认序列是“等间距”的，只考虑元素的顺序，不考虑它们的间距。 如果不等间距该怎么处理？ 例如不同时间拍摄的一组照片，每张都带一个时间戳，这种该怎么分析？ 用positional encoding标注时间吗？</p>
<p>corvusy：<br>我的想法是先用cnn抽取照片的represents，然后给represents加上时间戳做的positional encoding，再跑LSTM之类，这样可行吗？ 这种不等距位置编码该怎么做？ LSTM加位置编码有什么trick吗？ 谢谢沐神啦</p>
<p>跟李沐学AI：<br>回复 @corvusy :也许可以，也有可能时间不重要</p>
<hr>
<h1 id="4-1"><a href="#4-1" class="headerlink" title="4.1"></a>4.1</h1><p>专业英语讨论：</p>
<p>枯木流水残桥：<br>一个客观的工业人士还用中文在传道授业，非常感谢李沐老师。 现在中国其实最缺的就是工业视野。<br>我个人觉得中国人确实学习入门最好确实还是中文，听一遍课再去理解英文原版会轻松很多，而且英语我感觉，这门语言，信息量真的低，直观就是英语双语弹幕，中文大部分只有英语的很短一部分 便于中国人理解。 我英语考研也能答个80分。 看英文视频，很难理解透彻。</p>
<p>白痴陈教授：<br>特别专业性词汇太难了，其中逻辑性听英文太难了</p>
<p>吐槽的菠萝：<br>英语信息量大小，英语中文字幕长短和看英文视频难以理解，这三者之间没有必然的联系。<br>英语信息量的问题，有空可以看看CSPAN美国名参议员的发言，可以阳春白雪一句描述三件事，也可以下里巴人一件事用三句话重复五遍。 语言只是个交流的工具，全看怎么用，用来达到什么目的。<br>看英文视频难理解就有两方面了，视频内容好不好？ 观看者和讲述者的逻辑契合多少？ 同样九年义务教育也有不同的老师，有的适合把学生带上及格线、有的适合提高、有的适合带上满分、还有的擅长竞赛培训。 无非是学生需要适合自己的老师，老师需要针对学生看菜下饭。 [确定]</p>
<p>搬砖中的呆呆：<br>工业视野加一，很多学科的科研教学工业三者严重脱钩（生化环材[总督]）… 这样的模式中国高校怎么借鉴呢，感觉很多工业大佬工作日都在上班也没时间教书耶[微笑]学校也没这方面的机制，顶多有讲座</p>
<hr>
<p>霹雳吧啦Wz：<br>沐神，想问个关于数据集划分的问题(监督学习)。 您觉得训练自己数据集时有必要划分测试集吗？ 我自己使用时一般只会划分出训练集和验证集，因为都是随机取的，按概率来看即使多划分出一个测试集，他们的数据分布都是一样的。 所以我个人感觉就没有必要了。 像大型比赛中有专门的测试集我觉得测试集的数据分布可能和提供的验证集是不一样的，这样的测试集才有意义(还能防止作弊)。 望解惑[呲牙]</p>
<p>想要长胖的竹竿君：<br>有必要呀，你要在验证集上调超参数，这相当于人工对数据集进行训练学习和拟合了，测试集必须是在训练中完全没有见过的数据</p>
<p>约化质量：<br>导师，原来你也在测试集上调参[滑稽]</p>
<p>霹雳吧啦Wz：<br>回复 @约化质量 :我没有，别瞎说</p>
<hr>
<p>教地理的宋老师：<br>想趁双11买一个游戏本来玩深度学习，有什么推荐的吗？ 很多游戏本的测试都是游戏性能的，同样是3060不同的功耗游戏性能有很大的不同，对于深度学习是不是也可参考，另外3070的8G内存是否够用？ 一定需要上3080的16G内存吗？</p>
<p>黑吃黑的日常：<br>我1050也玩的挺开心的</p>
<p>阿斯普罗斯：<br>建议用免费的google colab，如果用不了的话，建议租云服务器，李沐老师有一期专门讲过的，使用 AWS 最便宜的 GPU 实例 - 动手学深度学习v2</p>
<p>魔法少女卡莎呀：<br>买个gpu云</p>
<p>南阳北雪_：<br>[总督]直接上服务器，我们实验室标配3060</p>
<p>搬砖中的呆呆：<br>推荐用云… 难一点的项目游戏本真的不够用，而且不方便… 如果入门倒是3070ti以下看自己的财力咯[总督]… 入门8G应该够了，但是做前沿的研究一般不够…</p>
<hr>
<p>学不会请打死我：<br>对于指标，这是我见过的最好的讲解</p>
<hr>
<h1 id="4-2"><a href="#4-2" class="headerlink" title="4.2"></a>4.2</h1><p>hylasier：<br>发现老师的PPT虽然除了图表就是文字，毫无装饰，但是文字的字体、大小、粗细、排版都非常讲究，图的绘制也很专业，所以看起来非常舒服</p>
<p>bili_2129092148：<br>学术ppt有规范，是这样的</p>
<hr>
<p>热爱生活的evan：<br>发现沐神在B站讲的内容和课程网站上的PPT内容相比有一些删减。。</p>
<hr>
<p>轩中科技公司董事长：<br>我想问一下啊，迁移学习和深度学习到底是个怎么样的关系呢，我了解的有点模糊不清啊[怪我咯][怪我咯]</p>
<p>嘉然今天吃时间晶体：<br>可以理解为已经训练好的神经网络再在相似的问题上再训练一次，减少训练消耗</p>
<hr>
<h1 id="4-3"><a href="#4-3" class="headerlink" title="4.3"></a>4.3</h1><h1 id="5-1"><a href="#5-1" class="headerlink" title="5.1"></a>5.1</h1><p>天真和伤感小说家：<br>李老师，我真的好喜欢你啊，为了你，我要发ACL！！！</p>
<p>跟李沐学AI：<br>👍 我没发过acl[笑哭]</p>
<hr>
<p>苏格拉什么都说：<br>每次看完沐神的视频，我都觉得自己能发AAAI，然后回到工位看着普通鬼画符的草稿，扔笔放弃！</p>
<hr>
<p>xiaofeixiazyh：<br>看了老师的讲解感觉懂了方差和偏差的概念，但是在训练过程如何区分模型是方差过大，还是偏差过大呢，是不是就是说过拟合就意味着方差过大？而欠拟合意味着偏差过大？</p>
<p>PS：学到知识点后勇于质疑与分析实际问题</p>
<hr>
<h1 id="5-2"><a href="#5-2" class="headerlink" title="5.2"></a>5.2</h1><p>xiaofeixiazyh：<br>（看完上一节的问题）看了老师的讲解感觉懂了方差和偏差的概念，但是在训练过程如何区分模型是方差过大，还是偏差过大呢，是不是就是说过拟合就意味着方差过大？而欠拟合意味着偏差过大？<br>接着这一节内容，我看到随机森林那个结果图的时候，不知道是如何解读？随着树的变多，模型的误差是降低的，这主要是降低了方差，那偏差有没有降低呢？并且随着树的增长，方差不会再增加，但是虽然train set 和 val set 的结果都收敛了，但它们之间仍然存在着较大的差距，这是因为模型的偏差吗？代表着模型结构的局限性吗？因为增加模型复杂度也不能进一步降低val set的误差了</p>
<p>Jason-cs：<br>（以下纯属个人理解）一般来说方差大的表现是过拟合，偏差比较大表现是欠拟合。按照bagging的结果显示，decision tree会随着#tree增加，误差下降。这里是由于方差下降，老师已经给出。而对于偏差，我觉得不会下降或者不能确定下降。因为本身bagging技术并没有增加模型复杂度，而是训练了多个同样复杂度的模型来聚合结果。所以如果本身模型就有underfit/high bias的现象，那么bagging本身应该也无法解决的。</p>
<hr>
<p>我是你是诶：<br>一开始感觉这门课有点鸡肋，现在越学越有感觉，感谢沐导！！（PS：附带代码的讲解太友好了，可以帮助理解理论</p>
<hr>
<h1 id="5-3"><a href="#5-3" class="headerlink" title="5.3"></a>5.3</h1><h1 id="5-4"><a href="#5-4" class="headerlink" title="5.4"></a>5.4</h1><p>Ocean_777：<br>亚马逊最近似乎出了个类似colab的平台[热词系列_吹爆]</p>
<p>渐奔明：<br>叫什么呀</p>
<p>吻鸠斩花熊：<br>回复 @渐奔明 :Amazon SageMaker Studio Lab</p>
<hr>
<p>sky-Muse：<br>沐神，前几天看精读论文，看到一个广告，然后我填了一下信息，收到了学堂在线的驭风计划信息，让我报名网课，我就报名了，后面才知道这个跟您没什么关系[喜极而泣]</p>
<p>跟李沐学AI：<br>要求退钱</p>
<hr>
<p>HorusEye：<br>在SageMaker Studio Lab首页看到了D2L[doge]</p>
<hr>
<p>RFCHANCE：<br>大佬，请问站立，下蹲，俯卧等等动作，每种动作统计一些关节和关节的距离作为数据，比如手腕和膝盖等等，手腕距离膝盖近的是下蹲。这可以用stacking吗？<br>PS：机器学习领域可大可小</p>
<hr>
<p>biginflation：<br>沐神讲的好 很多东西以前学生时代听过就过去了 工作后才知道简简单单一句话都是踩坑踩出来的血与泪啊</p>
<hr>
<h1 id="9-1"><a href="#9-1" class="headerlink" title="9.1"></a>9.1</h1><p>PS：数学类的课程放到后面讲</p>
<p>夜雨清歌寒：<br>请问，实用机器学习跟我们一般学的机器学习有什么区别吗？是因为算法模型都是工业界常用的嘛？</p>
<p>潜伏九段：<br>讲的知识点和方法论都是常用的</p>
<p>BAspring：<br>非也，其实就是这门课主要讲业界常用的好用的算法模型，并且附上一些经验之谈，比如”这样的模型在实践中一般都会xxx””我们一般会将xxx怎么怎么样”</p>
<hr>
<p>Goallow：<br>对于调超参 我有一点不是太明白，我现在将数据集划分为train validation test三个部分，利用train在validation调整超参数，调好用将train和validation合并再在test上做评估（边训练边评估，停止是用根据在test上的error），还是利用train训练model然后在validation判断模型收敛，然后将训练好的模型再在test上做评估</p>
<p>蒲公英的约定定：<br>train 训练，val验证调参，test得出最终模型表现</p>
<p>Goallow：<br>回复 @蒲公英的约定定 :我的意思是调完参后是吧 train和val合并，然后用test看generalization吗</p>
<hr>
<p>交通小吴：<br>我有一个疑惑，目前人工神经网络由于参数众多，寻找一个合适的参数非常耗时耗力，并且有些参数和参数之间内在又存在联系，导致我们不能单一评价一个参数越大越好还是越小越好，我们能否将模型参数作为训练集，结果作为target，进行二次建模，寻找最优解，从而找到最合适的模型参数，</p>
<p>尘埃1995：<br>这应该属于元学习吧</p>
<p>bili81569968286幻：<br>回复 @毫升五 :有人用其他启发式算法做过这个 还有贝叶斯优化 感兴趣可以看看</p>
<p>交通小吴：<br>大家认为我这个想法怎么样呢？？</p>
<p>PS：要有这种灵活的思维，善于交流</p>
<hr>
<p>低调的头帛书：<br>d2l.plot(x.detach(), y.detach(), ‘x’, ‘relu(x)’, figsize=(5, 2.5))<br>李老师，我用notebook跑这个代码老是报错，好像d2l画图都报错，但是我用colab就没有这个问题，不知道是什么原因</p>
<hr>
<h1 id="9-2"><a href="#9-2" class="headerlink" title="9.2"></a>9.2</h1><p>AliceKagiyama：<br>这个视频讲炼丹手法</p>
<hr>
<p>HorusEye：<br>请问，在acquisition function那里，沐神关于acquisition max的解释怎么理解?<br>1.值大代表不置信。请问这里acquisition值大代表的是什么不置信？<br>2.可能是目标函数比较高。这句话怎么理解？什么叫目标函数比较高？<br>谢谢！</p>
<p>多啦a木：</p>
<ol>
<li>你看那个图有个紫色的区域，在一个x上，紫色的区域越大就代表variance越大，也就是不置信，他说的acquisition max值的是紫色的区域要比较大</li>
<li>x轴的点代表一个数据点（一个超参数组合），y值代表的目标函数（可能是accuracy），所以我们肯定希望y值越大越好，也就是越高越好。<br>总的来说，获取函数就是要找一个（1）不那么置信的超参数组合（x轴的某个点）（2）y上的值要比较大（也就是预测目标函数的值比较大）</li>
</ol>
<hr>
<p>hylasier：<br>内容还是偏CV。老师能提一下NLP的内容就好了</p>
<p>爱睡觉的小依：<br>这些方法为什么偏CV，NLP难道有什么不同吗？感觉这些方法都比较通用啊</p>
<p>林木木特能吃：<br>这里讲的都是通用的机器学习的调参方法， 没有看到偏向CV的内容啊。这里讲的调参方法在NLP里也可以用</p>
<p>正弦波的圆环之理：<br>这个其实是自动化机器学习里的内容，应用非常广泛</p>
<hr>
<p>escbto：<br>老师，为啥autogluon在colab突然用不了了？</p>
<hr>
<p>翟鹤rin：<br>有人做过贝叶斯优化CNN的超参数吗。。网上这类代码太少了<br>2022-03-23 10:05</p>
<hr>
<h1 id="9-3"><a href="#9-3" class="headerlink" title="9.3"></a>9.3</h1><p>GragHack：<br>我是Google NAS team的，沐神这期内容跟我们组做的东西相关度好高[打call][打call]</p>
<p>L丶World：<br>我超，膜拜大佬</p>
<p>禺谷皓月：<br>大佬给跪了，求内推[doge]</p>
<p>GragHack：<br>回复 @禺谷皓月 :我们组在北美mtv，大佬有兴趣简历砸过来</p>
<p>吃king少年：<br>google有好多厉害的nas工作，而且都好贵，羡慕大佬。</p>
<p>長門不会来救你的：<br>人在北美 希望以后有机会去google做research intern！</p>
<p>本来可以靠脸吃饭：<br>大佬 我在做NAS这块 求交流！！！[打call]</p>
<hr>
<p>_ _ Texas_ _：<br>吃饭的时候看到这个</p>
<p>Google在研究NAS(神经架构搜索)的时候发了第一篇文章NEURAL ARCHITECTURE SEARCH WITH REINFORCEMENT LEARNING【ICLR’17】，<br>思路很直接，但直接跑cifar10用了800块gpu快一个月</p>
<p>后来又一篇Learning Transferable Architectures for Scalable Image Recognition【CVPR’18】。考虑用堆代搜，但是还是500块gpu跑了4天。</p>
<p>所以一个gpu都没有的我在这里诚借gpu资源【旺柴】</p>
<p>LoanMyBraintoYou：<br>500块钱而已</p>
<p>L丶World：<br>回复 @LoanMyBraintoYou :500块gpu不是500块钱啊</p>
<p>_Mirr_Or_：<br>Money is all you need[doge]</p>
<hr>
<p>garygegede的哥哥：<br>沐神，本人目前某中流985大三学生，读研导师为nlp方向的。目前不知道要做哪些努力，感觉本科期间就学了一下开发，还有机器学习理论基础，目前没啥方向，不知道学啥好[笑哭]和发论文的水平还有好大差距的感觉[笑哭][笑哭][笑哭]</p>
<p>多啦a木：<br>nlp也有很多方向啊 NER，对话机器人、知识图谱等等都是很常用的，看你导师哪一方面比较强啊</p>
<p>六个币一次：<br>上kaggle、天池打比赛，比赛打多了自己的想法就有了</p>
<hr>
<h1 id="10-1"><a href="#10-1" class="headerlink" title="10.1"></a>10.1</h1><p>我转起来了TnT：<br>沐神读论文系列能不能出一期推荐系统的呀，顺便可以推荐一下现在推荐系统的研究方向嘛</p>
<p>唐九先生：<br>授人以鱼不如授人以渔，兄弟贪了</p>
<p>我转起来了TnT：<br>回复 @唐九先生 :道理都懂也一直在看论文，毕竟沐神站得高看得远，有的时候可能提一句就会对我有很大帮助，我也只是问一问万一沐神回我了呢[脱单doge][脱单doge][脱单doge]</p>
<hr>
<p>sylviiaaaa：<br>沐神，这个课还会继续更新吗？<br>2022-02-12 11:44</p>
<p>跟李沐学AI：<br>会的。我在考虑加一些代码进去</p>
<hr>
<h1 id="11-1"><a href="#11-1" class="headerlink" title="11.1"></a>11.1</h1><h1 id="11-2"><a href="#11-2" class="headerlink" title="11.2"></a>11.2</h1><p>NLP从入门到放弃：<br>老师讲的超棒！！Nlp这块预训练大家可以直接用抱抱脸的transformers库，轻松上手。微调这块关于文本分类邱老师组有个关于文本分类微调各种实验细节，也超有用</p>
<p>1横槊赋诗2：<br>上次想用simbert模型，但是抱抱库没找到</p>
<p>凸凸凸凸凸凸凸豆：<br>标记</p>
<p>将劝酒：<br>请问能给个链接吗</p>
<hr>
<p>账号已注销：<br>新手学习之前课程前，向大佬们请教下这个问题是不是可以用深度学习解决？</p>
<p>关于预测墨水搭配方式及浓度比例的问题。<br>1、印刷品的颜色可以用(red,green,blue)三原色表示。<br>2、已知手上有20种墨水ABCEFGHIJL…<br>3、已知手上有5种不同材质的纸张m n o p q<br>4、已知在某种材质上，打印一种颜色X，比如小猪粉色rgb(255,102,255) ，需要多种墨水配合比如 A0.5% G0.4% L0.12%三种墨水及不同的浓度。字母表示墨水型号，数字表示浓度，三个组合表示这个搭配可以成功打印出某种颜色Xrgb(255,102,255)。一种颜色墨水不超过5种。<br>5、这样的已知的成功组合有很多，或者可以整理好true数据库，并不断补充，大约几千条。单个墨水A不同浓度和不同材质下打出来的颜色RGB值也可以花点时间录入进去。<br>6、除了材质，还有机器的温度T也会影响结果。</p>
<p>现在客户来了一种颜色比如Y(102,51,204)和一种确定的材质m：<br>1、如何推荐墨水组合和用多少浓度和机器温度能打出这种颜色。<br>2、或者人工(肉眼经验)大致通过确定 ABG或EF墨水组合先，求这个特定组合的墨水浓度。</p>
<p>就是这个反向推导问题，是不是可以用深度学习解决呢？</p>
<p>谢谢大佬！</p>
<p>TAGAKI何夜无月：<br>个人感觉这个问题综合它的feature和数据量来看的话，或许传统的机器学习算法比如决策树，xgboost之类的可能效果会更好，当然用深度学习也不是不行</p>
<p>fool1914：<br>深度学习实际上在很多小数据集任务上可能不一定有传统机器学习我觉得好。因为你不需要考虑很多特征工程。不过要注意的是你这个属于少样本学习，属于比较复杂的情况讨论，few-shot learning，我预计你的最后效果不会很好。为什么这么说，因为你一种颜色的墨水组合我估计比较少，数据量太小，可能xrgb这个颜色组合你最多就6到7种搭配，这样真的挺难搞的。除非你一种颜色，例如xrgb的搭配数据量至少100条吧条，才可能效果好。</p>
<p>fool1914：<br>深度学习还有一个大问题就是解释性差，这个问题我觉得你拿决策树/随机森林做效果是最好的，你还能反向推理向量的取值。</p>
<hr>
<p>辛迪朋：<br>不懂就问，我可以对BERT模型中间层的输出插一个新的层再继续丢到BERT后续的层输出来针对特定的任务做训练吗？这样BERT预训练的参数是否还有意义？</p>
<hr>

      
      
    </div>
    <footer class="article-footer">
      <ul class="article-meta">
        <li>
          <span class="label">Published Date:</span>
          <a href="/2022/08/24/%E3%80%90%E6%9D%8E%E6%B2%90B%E7%AB%99%E8%AF%BE%E7%A8%8B%EF%BC%9A%E5%AE%9E%E6%88%98%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E3%80%91%E3%80%90%E6%9C%89%E6%84%9F%E8%A7%A6%E7%9A%84%E8%AF%84%E8%AE%BA%E3%80%91/" class="article-date">
  <time class="dt-published" datetime="2022-08-24T14:31:29.794Z" itemprop="datePublished">2022-08-24</time>
</a>

        </li>
        
          <li>
            <span class="label">Category:</span>
            
  <div class="article-category">
    <a class="article-category-link" href="/categories/Mu-Li/">Mu Li</a>/<a class="article-category-link" href="/categories/Mu-Li/%E7%BB%8F%E9%AA%8C%E8%B4%B4/">经验贴</a>
  </div>


          </li>
        
        
          <li>
            <span class="label">Tag:</span>
            
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Mu-Li-%E7%BB%8F%E9%AA%8C/" rel="tag">Mu Li - 经验</a></li></ul>


          </li>
        
        <hr/>
      </ul>
    </footer>
  </div>
  
    
<nav id="article-nav" class="article-nav">
  
    <a href="/2022/09/15/%E3%80%90Wallpaper%E3%80%91%E3%80%90Sharing%E3%80%91/" id="article-nav-newer" class="article-nav-link-wrap newer">
      <strong class="article-nav-caption">Newer</strong>
      <div class="article-nav-title">
        
          【Wallpaper】【Sharing】
        
      </div>
    </a>
  
  
    <a href="/2022/08/01/%E3%80%90%E7%BB%8F%E9%AA%8C%E8%B4%B4%E3%80%91/" id="article-nav-older" class="article-nav-link-wrap older">
      <strong class="article-nav-caption">Older</strong>
      <div class="article-nav-title">【经验贴】</div>
    </a>
  
</nav>


  
</article>










      </div>
      
    <footer id="footer" class="post-footer footer">
      
      <hr/>
      <div id="footerContent" class="footer-content">
        <p>ipsum dolor sit amet, <strong>consectetur adipiscing elit.</strong> Fusce eget urna vitae velit <em>eleifend interdum at ac nisi. In nec ligula lacus. Cum sociis natoque</em> penatibus et magnis dis parturient montes, nascetur ridiculus mus. Sed eu cursus erat, ut dapibus quam. Post</p>


      </div>
    </footer>

      








<script src="https://cdn.jsdelivr.net/npm/jquery@3/dist/jquery.min.js"></script>


<script src="https://cdn.jsdelivr.net/npm/clipboard@2/dist/clipboard.min.js"></script>



  
<script src="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@3/dist/jquery.fancybox.min.js"></script>




<script src="/js/typing.js"></script>

<!--[if lt IE 9]>
<script src="https://cdn.jsdelivr.net/npm/html5shiv@3/dist/html5shiv.min.js"></script>
<![endif]-->







    </div>
  </body>
</html>
