<!DOCTYPE html>
<html lang="zh-CN">

<head>

  <!-- Minima -->
  <!-- Hexo theme created by @adisaktijrs -->

  <!-- Basic Page Needs
  –––––––––––––––––––––––––––––––––––––––––––––––––– -->
  <meta charset="utf-8">

  
  <title>【R】【决策树】【随机森林】</title>
  
  <link rel="canonical" href="https://guairen-7.github.io/2022/04/19/%E3%80%90R%E3%80%91%E3%80%90%E5%86%B3%E7%AD%96%E6%A0%91%E3%80%91%E3%80%90%E9%9A%8F%E6%9C%BA%E6%A3%AE%E6%9E%97%E3%80%91/">
  
  <meta name="description" content="实验思维导图 1.决策树–ctree()–iris1.1 数据1.1.1 程序包加载简洁安装程序包：将提供的程序包手动复制到R的镜像下的library下：例如我的路径：D:\R-4.1.3\library，将重复的程序包替换即可。底部提供资料 1234567891011121314library(I">
  
  
  <meta name="author" content="AXDLMG7">
  
  
  
  <meta property="og:site_name" content="AXDLMG7&#39;s Blog" />
  <meta property="og:type" content="article" />
  <meta property="og:title" content="【R】【决策树】【随机森林】" />
  
  <meta property="og:description" content="实验思维导图 1.决策树–ctree()–iris1.1 数据1.1.1 程序包加载简洁安装程序包：将提供的程序包手动复制到R的镜像下的library下：例如我的路径：D:\R-4.1.3\library，将重复的程序包替换即可。底部提供资料 1234567891011121314library(I">
  
  <meta property="og:url" content="https://guairen-7.github.io/2022/04/19/%E3%80%90R%E3%80%91%E3%80%90%E5%86%B3%E7%AD%96%E6%A0%91%E3%80%91%E3%80%90%E9%9A%8F%E6%9C%BA%E6%A3%AE%E6%9E%97%E3%80%91/" />

  <meta name="twitter:card" content="summary_large_image">
  <meta name="twitter:title" content="【R】【决策树】【随机森林】">
  
  <meta name="twitter:description" content="实验思维导图 1.决策树–ctree()–iris1.1 数据1.1.1 程序包加载简洁安装程序包：将提供的程序包手动复制到R的镜像下的library下：例如我的路径：D:\R-4.1.3\library，将重复的程序包替换即可。底部提供资料 1234567891011121314library(I">
  
  
  
  
  <meta name="twitter:url" content="https://guairen-7.github.io/2022/04/19/%E3%80%90R%E3%80%91%E3%80%90%E5%86%B3%E7%AD%96%E6%A0%91%E3%80%91%E3%80%90%E9%9A%8F%E6%9C%BA%E6%A3%AE%E6%9E%97%E3%80%91/" />

  <!-- Mobile Specific Metas
  –––––––––––––––––––––––––––––––––––––––––––––––––– -->
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <!-- Preload fonts
  –––––––––––––––––––––––––––––––––––––––––––––––––– -->
  <link rel="preload" href="/fonts/dm-serif-display-v4-latin-regular.woff2" as="font" type="font/woff2" crossorigin>
  <link rel="preload" href="/fonts/inter-v2-latin-regular.woff2" as="font" type="font/woff2" crossorigin>

  <!-- CSS
  –––––––––––––––––––––––––––––––––––––––––––––––––– -->
  
<link rel="stylesheet" href="/css/normalize.css">

  
<link rel="stylesheet" href="/css/skeleton.css">

  
<link rel="stylesheet" href="/css/custom.css">

  
<link rel="stylesheet" href="/css/prism-dark.css">

  
<link rel="stylesheet" href="/css/prism-line-numbers.css">

  <!-- User css -->
  
  
<link rel="stylesheet" href="/css/user.css">

  

  <!-- Favicon
  –––––––––––––––––––––––––––––––––––––––––––––––––– -->
  <link rel="icon" type="image/png" href="/images/favicon.png">

  <!-- Custom Theme Color Style
  –––––––––––––––––––––––––––––––––––––––––––––––––– -->
  <style>
  a:not(.icon) {
    text-decoration-color: #0FA0CE;
    background-image: linear-gradient(
      to bottom,
      rgba(0, 0, 0, 0) 50%,
      #0FA0CE 50%
    );
  }
  blockquote {
    border-left: 8px solid #0FA0CE;
  }
  .nanobar .bar {
    background: #0FA0CE;
  }
  .button.button-primary:hover,
  button.button-primary:hover,
  input[type="submit"].button-primary:hover,
  input[type="reset"].button-primary:hover,
  input[type="button"].button-primary:hover,
  .button.button-primary:focus,
  button.button-primary:focus,
  input[type="submit"].button-primary:focus,
  input[type="reset"].button-primary:focus,
  input[type="button"].button-primary:focus {
    background-color: #0FA0CE;
    border-color: #0FA0CE;
  }
  input[type="email"]:focus,
  input[type="number"]:focus,
  input[type="search"]:focus,
  input[type="text"]:focus,
  input[type="tel"]:focus,
  input[type="url"]:focus,
  input[type="password"]:focus,
  textarea:focus,
  select:focus {
    border: 1px solid #0FA0CE;
  }
</style>

  <!-- Google Analytics (With Privacy Settings On)
  –––––––––––––––––––––––––––––––––––––––––––––––––– -->
  

  
  <script src="/js/pic.min.js" defer></script>
  

  
  <script src="/js/search.min.js" defer></script>
  

<meta name="generator" content="Hexo 5.4.1"></head>

<body>
  <div class="container">
    <div class="row">
      <div>

        <div class="row">
  <div class="two columns" style="max-width: 50px">
    <h1 class="mt-2 mode">
      <div onclick=setDarkMode(true) id="darkBtn">🌑</div>
      <div onclick=setDarkMode(false) id="lightBtn" class=hidden>☀️</div>
      <script >
        if (localStorage.getItem('preferredTheme') == 'dark') {
          setDarkMode(true)
        }
        function setDarkMode(isDark) {
          var darkBtn = document.getElementById('darkBtn')
          var lightBtn = document.getElementById('lightBtn')
          if (isDark) {
            lightBtn.style.display = "block"
            darkBtn.style.display = "none"
            localStorage.setItem('preferredTheme', 'dark');
          } else {
            lightBtn.style.display = "none"
            darkBtn.style.display = "block"
            localStorage.removeItem('preferredTheme');
          }
          document.body.classList.toggle("darkmode");
        }
      </script>
    </h1>
  </div>

  <div class="six columns ml-1">
    <h1 class="mt-2">
      Hi Folks.
    </h1>
  </div>

  <div class="twelve columns">
    <div class="row">
      <div class="nine columns left">
        <a href="/">Home</a>
        
          
          <a href="/Works" class="ml">Works</a>
          
        
          
          <a href="/About" class="ml">About</a>
          
        
        
          
            <a href="mailto:test@test.test" target="_blank" class="ml">Email</a>
          
        
      </div>
    </div>
    <hr style="margin-bottom: 2.6rem">
  </div>
</div>

        <div class="trans">
            <h2>【R】【决策树】【随机森林】</h2>

  <h1 id="实验思维导图"><a href="#实验思维导图" class="headerlink" title="实验思维导图"></a>实验思维导图</h1><p><img src="https://img-blog.csdnimg.cn/808441d7c92d40d48204ccdc5b4438f3.png" alt="在这里插入图片描述"></p>
<h1 id="1-决策树–ctree-–iris"><a href="#1-决策树–ctree-–iris" class="headerlink" title="1.决策树–ctree()–iris"></a>1.决策树–ctree()–iris</h1><h2 id="1-1-数据"><a href="#1-1-数据" class="headerlink" title="1.1 数据"></a>1.1 数据</h2><h3 id="1-1-1-程序包加载"><a href="#1-1-1-程序包加载" class="headerlink" title="1.1.1 程序包加载"></a>1.1.1 程序包加载</h3><p>简洁安装程序包：将提供的程序包手动复制到R的镜像下的library下：<br>例如我的路径：D:\R-4.1.3\library，将重复的程序包替换即可。<strong>底部提供资料</strong></p>
<figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">library(ISLR)</span><br><span class="line">library(TH.data)</span><br><span class="line">library(MASS)</span><br><span class="line">library(multcomp)</span><br><span class="line">library(matrixStats)</span><br><span class="line">library(libcoin)</span><br><span class="line">library(survival)</span><br><span class="line">library(coin)</span><br><span class="line">library(zoo)</span><br><span class="line">library(strucchange)</span><br><span class="line">library(modeltools)</span><br><span class="line">library(mvtnorm)</span><br><span class="line">library(party)</span><br><span class="line">library(grid)</span><br></pre></td></tr></table></figure>
<figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#party包中的ctree()：</span></span><br><span class="line"><span class="comment">#1.用于创建决策树</span></span><br><span class="line"><span class="comment">#2.提供用于控制决策树训练的几个参数，例如 midSplit、Min Busket、MaxSurrogate 和 MaxDepth</span></span><br></pre></td></tr></table></figure>
<h3 id="1-1-2-数据集探索"><a href="#1-1-2-数据集探索" class="headerlink" title="1.1.2 数据集探索"></a>1.1.2 数据集探索</h3><figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#查看iris数据集</span></span><br><span class="line">str(iris)</span><br></pre></td></tr></table></figure>
<p><img src="https://img-blog.csdnimg.cn/5537ed985a8b4eac800eac566e0c6399.png" alt="在这里插入图片描述"><br>数据集大小：150条数据、5个变量</p>
<p>在许多的科研著作中都在iris数据集上做分类操作。该数据集由3种不同类型的鸢尾花的50个样本数据构成。其中的一个种类与另外两个种类是线性可分离的，后两个种类是非线性可分离的。这个数据集包含了5个属性：</p>
<ul>
<li><p>Sepal.Length（花萼长度），单位是cm。</p>
</li>
<li><p>Sepal.Width（花萼宽度），单位是cm。</p>
</li>
<li><p>Petal.Length（花瓣长度），单位是cm。</p>
</li>
<li><p>Petal.Width（花瓣宽度），单位是cm。<br>种类：</p>
</li>
<li><p>Iris Setosa（山鸢尾）</p>
</li>
<li><p>Iris Versicolour（杂色鸢尾）</p>
</li>
<li><p>Iris Virginica（维吉尼亚鸢尾）</p>
<h3 id="1-1-3-数据集拆分"><a href="#1-1-3-数据集拆分" class="headerlink" title="1.1.3 数据集拆分"></a>1.1.3 数据集拆分</h3><figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#拆分数据集</span></span><br><span class="line">ind &lt;- sample(<span class="number">2</span>, nrow(iris), replace= <span class="literal">TRUE</span>, prob=<span class="built_in">c</span>(<span class="number">0.7</span>, <span class="number">0.3</span>))</span><br><span class="line"><span class="comment">#同ind &lt;- sample(x = 2,size = nrow(iris),replace=TRUE,prob = c(0.7,0.3))</span></span><br><span class="line">trainData &lt;- iris[ind==<span class="number">1</span>,]</span><br><span class="line">testData &lt;- iris[ind==<span class="number">2</span>,]</span><br></pre></td></tr></table></figure>
<figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#sample():</span></span><br><span class="line"><span class="comment">#x即拆分为两部分</span></span><br><span class="line"><span class="comment">#size即抽样大小</span></span><br><span class="line"><span class="comment">#replace = TRUE</span></span><br><span class="line"><span class="comment">#有放回抽样，“replace”就是重复的意思，即可以重复对元素进行抽样，也就是所谓的有放回抽样。</span></span><br><span class="line"><span class="comment">#prob即“probability”（概率）</span></span><br></pre></td></tr></table></figure>
<figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#查看拆分后的数据</span></span><br><span class="line">str(trainData)</span><br><span class="line">str(testData)</span><br></pre></td></tr></table></figure>
<p><img src="https://img-blog.csdnimg.cn/55b07dded52b4c91bd819bd5d2cb5516.png" alt="在这里插入图片描述"></p>
<h2 id="1-2-训练"><a href="#1-2-训练" class="headerlink" title="1.2 训练"></a>1.2 训练</h2><h3 id="1-2-1-设置因变量、自变量"><a href="#1-2-1-设置因变量、自变量" class="headerlink" title="1.2.1 设置因变量、自变量"></a>1.2.1 设置因变量、自变量</h3><figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#myFormula指定了Species为目标变量，其余所有变量为自变量</span></span><br><span class="line">myFormula &lt;- Species ~ Sepal.Length + Sepal.Width + Petal.Length + Petal.Width</span><br></pre></td></tr></table></figure>
<h3 id="1-2-2-决策树建模"><a href="#1-2-2-决策树建模" class="headerlink" title="1.2.2 决策树建模"></a>1.2.2 决策树建模</h3><figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">iris_ctree &lt;- ctree(myFormula, data=trainData)</span><br></pre></td></tr></table></figure>
<h3 id="1-2-3-查看训练结果"><a href="#1-2-3-查看训练结果" class="headerlink" title="1.2.3 查看训练结果"></a>1.2.3 查看训练结果</h3><figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#检查预测结果</span></span><br><span class="line">table(predict(iris_ctree),trainData$Species)</span><br></pre></td></tr></table></figure>
<p><img src="https://img-blog.csdnimg.cn/34ce1651b5c34e06a2837227391ff4f3.png" alt="在这里插入图片描述"></p>
<figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#Setosa鸢尾花：37条</span></span><br><span class="line"><span class="comment">#Versicolour鸢尾花：34条</span></span><br><span class="line"><span class="comment">#Virginica鸢尾花：34条</span></span><br><span class="line"><span class="comment">#既是Versicolour鸢尾花也是Virginica鸢尾花：4条</span></span><br><span class="line"><span class="comment">#共109条数据</span></span><br></pre></td></tr></table></figure>
<figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#查看已经训练过的决策树</span></span><br><span class="line">print(iris_ctree)</span><br></pre></td></tr></table></figure>
<p><img src="https://img-blog.csdnimg.cn/d5ad5189b6a64d409d96aafaad589666.png" alt="在这里插入图片描述"></p>
<h3 id="1-2-4-绘制决策树"><a href="#1-2-4-绘制决策树" class="headerlink" title="1.2.4 绘制决策树"></a>1.2.4 绘制决策树</h3><figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#绘制决策树</span></span><br><span class="line">plot(iris_ctree)</span><br></pre></td></tr></table></figure>
<p><img src="https://img-blog.csdnimg.cn/348ccad4045644fa9334df41da7cb674.png" alt="在这里插入图片描述"></p>
<figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#决策树简化形式</span></span><br><span class="line">plot(iris_ctree,type=<span class="string">&quot;simple&quot;</span>)</span><br></pre></td></tr></table></figure>
<p><img src="https://img-blog.csdnimg.cn/e3330d8a3bf243fd8121ac57aac27ce7.png" alt="在这里插入图片描述"><br>每个叶子节点的条形图显示一个实例被划分到某个种类到概率：</p>
</li>
<li><p>节点2“n=37,y=(1,0,0)”,表示该节点包含37个训练实例,并且所有实例都属于“setosa”</p>
</li>
<li><p>节点6“n=7,y=(0,0.429,0.571)”,表示该节点包含7个训练实例,42.9%的实例属于“Versicolour”,57.1%的实例属于“Virginica”，7×57.1%≈4,对应了4.1节的“既是Versicolour鸢尾花也是Virginica鸢尾花：4条”</p>
<h2 id="1-3-预测"><a href="#1-3-预测" class="headerlink" title="1.3 预测"></a>1.3 预测</h2><h3 id="1-3-1-测试集预测"><a href="#1-3-1-测试集预测" class="headerlink" title="1.3.1 测试集预测"></a>1.3.1 测试集预测</h3><figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#predict()进行预测</span></span><br><span class="line">testPred &lt;- predict(iris_ctree,newdata = testData)</span><br><span class="line"></span><br><span class="line"><span class="comment">#查看预测结果</span></span><br><span class="line">table(testPred,testData$Species)</span><br></pre></td></tr></table></figure>
<p><img src="https://img-blog.csdnimg.cn/3618ef0eb4bd40e48bfe85d81d776523.png" alt="在这里插入图片描述"></p>
<h3 id="1-3-2-预测结果分析"><a href="#1-3-2-预测结果分析" class="headerlink" title="1.3.2 预测结果分析"></a>1.3.2 预测结果分析</h3><figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#Setosa鸢尾花：13条</span></span><br><span class="line"><span class="comment">#Versicolour鸢尾花：11条</span></span><br><span class="line"><span class="comment">#Virginica鸢尾花：15条</span></span><br><span class="line"><span class="comment">#既是Versicolour鸢尾花也是Virginica鸢尾花：1条</span></span><br><span class="line"><span class="comment">#既是Virginica鸢尾花也是Versicolour鸢尾花：1条</span></span><br><span class="line"><span class="comment">#共41条数据</span></span><br></pre></td></tr></table></figure>
<h1 id="2-决策树–rpart-–bodyfat"><a href="#2-决策树–rpart-–bodyfat" class="headerlink" title="2.决策树–rpart()–bodyfat"></a>2.决策树–rpart()–bodyfat</h1><h2 id="2-1-数据"><a href="#2-1-数据" class="headerlink" title="2.1 数据"></a>2.1 数据</h2><h3 id="2-1-1-程序包加载"><a href="#2-1-1-程序包加载" class="headerlink" title="2.1.1 程序包加载"></a>2.1.1 程序包加载</h3><figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">install.package(<span class="string">&#x27;rpart&#x27;</span>)</span><br><span class="line">library(rpart)</span><br><span class="line"><span class="comment">#rpart这个包被用来在&#x27;bodyfat&#x27;这个数据集的基础上建立决策树。</span></span><br><span class="line"><span class="comment">#函数raprt()可以建立一个决策树，并且可以选择最小误差的预测。</span></span><br></pre></td></tr></table></figure>
<h3 id="2-1-2-数据集探索"><a href="#2-1-2-数据集探索" class="headerlink" title="2.1.2 数据集探索"></a>2.1.2 数据集探索</h3><figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">data(<span class="string">&quot;bodyfat&quot;</span>, package = <span class="string">&quot;TH.data&quot;</span>)</span><br><span class="line"><span class="built_in">dim</span>(bodyfat)  <span class="comment">#查看数据尺寸</span></span><br><span class="line"><span class="built_in">attributes</span>(bodyfat)  <span class="comment">#查看数据变量名、行名称、数据类型等</span></span><br><span class="line">bodyfat[<span class="number">1</span>:<span class="number">5</span>,]  <span class="comment">#查看前五行数据</span></span><br></pre></td></tr></table></figure>
<p><img src="https://img-blog.csdnimg.cn/30b2b5b5c2514475b4601cd61dc56f96.png" alt="在这里插入图片描述">可以看到数据集有71条数据，包括10个变量：</p>
</li>
<li><p>age：年龄。</p>
</li>
<li><p>DEXfat：以DXA计算的体脂重，响应变量。</p>
</li>
<li><p>waistcirc：腰围。</p>
</li>
<li><p>hipcirc：臀围。</p>
</li>
<li><p>elbowbreadth：肘宽。</p>
</li>
<li><p>kneebreadth：膝宽。</p>
</li>
<li><p>anthro3a：三项人体测量的对数和。</p>
</li>
<li><p>anthro3b：三项人体测量的对数和。</p>
</li>
<li><p>anthro3c：三项人体测量的对数和。</p>
</li>
<li><p>anthro4：三项人体测量的对数和。</p>
</li>
</ul>
<h3 id="2-1-3-数据集拆分"><a href="#2-1-3-数据集拆分" class="headerlink" title="2.1.3 数据集拆分"></a>2.1.3 数据集拆分</h3><figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">set.seed(<span class="number">1234</span>)  <span class="comment">#设置随机种子</span></span><br><span class="line">ind &lt;- sample(<span class="number">2</span>, nrow(bodyfat), replace=<span class="literal">TRUE</span>, prob=<span class="built_in">c</span>(<span class="number">0.7</span>, <span class="number">0.3</span>))</span><br><span class="line">bodyfat.train &lt;- bodyfat[ind==<span class="number">1</span>,]</span><br><span class="line">bodyfat.test &lt;- bodyfat[ind==<span class="number">2</span>,]</span><br><span class="line"></span><br><span class="line"><span class="comment">#查看拆分后的数据</span></span><br><span class="line">str(bodyfat.train)</span><br><span class="line">str(bodyfat.test)</span><br></pre></td></tr></table></figure>
<p><img src="https://img-blog.csdnimg.cn/56bd16dff5c7443dae2501d43c940b99.png" alt="在这里插入图片描述"></p>
<h2 id="2-2-训练"><a href="#2-2-训练" class="headerlink" title="2.2 训练"></a>2.2 训练</h2><h3 id="2-2-1-设置因变量、自变量"><a href="#2-2-1-设置因变量、自变量" class="headerlink" title="2.2.1 设置因变量、自变量"></a>2.2.1 设置因变量、自变量</h3><figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#将DEXfat设置为因变量，age、waistcirc、hipcirc、elbowbreadth、kneebreadth为自变量</span></span><br><span class="line">myFormula &lt;- DEXfat ~ age + waistcirc + hipcirc + elbowbreadth + kneebreadth</span><br></pre></td></tr></table></figure>
<h3 id="2-2-2-决策树建模"><a href="#2-2-2-决策树建模" class="headerlink" title="2.2.2 决策树建模"></a>2.2.2 决策树建模</h3><figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">bodyfat_rpart &lt;- rpart(myFormula, data = bodyfat.train,control = rpart.control(minsplit = <span class="number">10</span>))</span><br><span class="line"><span class="comment">#函数格式rpart(formula, data, weights, subset, na.action = na.rpart, method,</span></span><br><span class="line">        model = <span class="literal">FALSE</span>, x = <span class="literal">FALSE</span>, y = <span class="literal">TRUE</span>, parms, control, cost, ...)</span><br><span class="line"><span class="comment">#control设置决策树的参数</span></span><br><span class="line"><span class="comment">#minsplit用于指定节点的最小样本量，默认为20.当节点样本量小于指定值时将不再继续分组</span></span><br></pre></td></tr></table></figure>
<h3 id="2-2-3-查看训练结果"><a href="#2-2-3-查看训练结果" class="headerlink" title="2.2.3 查看训练结果"></a>2.2.3 查看训练结果</h3><figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">print(bodyfat_rpart)</span><br></pre></td></tr></table></figure>
<p><img src="https://img-blog.csdnimg.cn/0a0df2c1fc504001b0338ed97a1ebf66.png" alt="在这里插入图片描述"></p>
<h3 id="2-2-4-绘制决策树"><a href="#2-2-4-绘制决策树" class="headerlink" title="2.2.4 绘制决策树"></a>2.2.4 绘制决策树</h3><figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">plot(bodyfat_rpart)</span><br><span class="line"></span><br><span class="line"><span class="comment">#为决策树添加文本标签</span></span><br><span class="line">text(bodyfat_rpart, use.n=<span class="built_in">T</span>)</span><br></pre></td></tr></table></figure>
<p><img src="https://img-blog.csdnimg.cn/cc149eb768c74a7e9969b370513fc4de.png" alt="在这里插入图片描述"></p>
<h2 id="2-3-优化"><a href="#2-3-优化" class="headerlink" title="2.3 优化"></a>2.3 优化</h2><h3 id="2-3-1-优化模型（剪枝）"><a href="#2-3-1-优化模型（剪枝）" class="headerlink" title="2.3.1 优化模型（剪枝）"></a>2.3.1 优化模型（剪枝）</h3><figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#选择预测误差最小值的预测树，从而优化模型</span></span><br><span class="line">opt &lt;- which.min(bodyfat_rpart$cptable[,<span class="string">&quot;xerror&quot;</span>])</span><br><span class="line">cp &lt;- bodyfat_rpart$cptable[opt, <span class="string">&quot;CP&quot;</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment">#prune函数可以实现最小代价复杂度剪枝法</span></span><br><span class="line">bodyfat_prune &lt;- prune(bodyfat_rpart, cp = cp)</span><br><span class="line"><span class="comment">#cp为复杂度系数，上面的办法选择具有最小xerror的cp的办法</span></span><br></pre></td></tr></table></figure>
<p>这里最开始不太明白“bodyfat_rpart$cptable”，简单查看一下：<br><img src="https://img-blog.csdnimg.cn/b443f1a83c3f45d9876fd9ce941a3838.png" alt="在这里插入图片描述"><br><img src="https://img-blog.csdnimg.cn/3e93d9c26b5c4965926ab499acf168dd.png" alt="在这里插入图片描述"></p>
<h2 id="2-3-2-绘制决策树（剪枝后）"><a href="#2-3-2-绘制决策树（剪枝后）" class="headerlink" title="2.3.2 绘制决策树（剪枝后）"></a>2.3.2 绘制决策树（剪枝后）</h2><figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#绘制决策树</span></span><br><span class="line">plot(bodyfat_prune)</span><br><span class="line"><span class="comment">#添加文本标签</span></span><br><span class="line">text(bodyfat_prune, use.n=<span class="built_in">T</span>)</span><br></pre></td></tr></table></figure>
<p><img src="https://img-blog.csdnimg.cn/87a28d64976f40379d54823bccb641db.png" alt="在这里插入图片描述"><br>对比结果就会发现，优化模型后，就是将hipcirc&lt;99.5这个分层给去掉了，也许是因为这个分层没有必要，<strong>可以思考一下为什么选择预测误差最小的结果的决策树的分层反而没有那么细</strong>。</p>
<h2 id="2-4-预测"><a href="#2-4-预测" class="headerlink" title="2.4 预测"></a>2.4 预测</h2><h3 id="2-4-1-测试集预测"><a href="#2-4-1-测试集预测" class="headerlink" title="2.4.1 测试集预测"></a>2.4.1 测试集预测</h3><figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">DEXfat_pred &lt;- predict(bodyfat_prune, newdata=bodyfat.test)</span><br></pre></td></tr></table></figure>
<h3 id="2-4-2-预测值的极值"><a href="#2-4-2-预测值的极值" class="headerlink" title="2.4.2 预测值的极值"></a>2.4.2 预测值的极值</h3><figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">xlim &lt;- <span class="built_in">range</span>(bodyfat$DEXfat)</span><br><span class="line"></span><br><span class="line"><span class="comment">#绘制散点图</span></span><br><span class="line">plot(DEXfat_pred ~ DEXfat,data=bodyfat.test,xlab=<span class="string">&quot;Observed&quot;</span>,ylab=<span class="string">&quot;Predicted&quot;</span>,ylim=xlim,xlim=xlim)</span><br><span class="line"><span class="comment">#此处plot(纵坐标数据~横坐标数据，数据，横坐标名称，纵坐标名称，纵坐标轴数值取bodyfat$DEXfat范围，横坐标轴数值取bodyfat$DEXfat范围)</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#添加对角线</span></span><br><span class="line">abline(a=<span class="number">0</span>, b=<span class="number">1</span>)</span><br></pre></td></tr></table></figure>
<p><img src="https://img-blog.csdnimg.cn/9e788246055d49358e4de2bd2a68f042.png" alt="在这里插入图片描述"><br>优化后的决策树将会用来预测，预测的结果会与实际的值进行对比。<br>上面的代码中，使用函数abline()绘制一条斜线。<br>一个好的模型的预测值应该是约接近真实值越好，也就是说大部分的点应该落在斜线上面或者在斜线附近。</p>
<h1 id="3-随机森林–randomForest-–iris"><a href="#3-随机森林–randomForest-–iris" class="headerlink" title="3.随机森林–randomForest()–iris"></a>3.随机森林–randomForest()–iris</h1><h2 id="3-1-程序包加载"><a href="#3-1-程序包加载" class="headerlink" title="3.1 程序包加载"></a>3.1 程序包加载</h2><figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">install.packages(<span class="string">&#x27;randomForest&#x27;</span>)</span><br><span class="line">library(randomForest)</span><br></pre></td></tr></table></figure>
<p>我们使用包randomForest并利用鸢尾花数据建立一个预测模型，包里面的randomForest()函数有两点不足：</p>
<ul>
<li>第一，<strong>它不能处理缺失值</strong>，使得用户必须在使用该函数之前填补这些缺失值</li>
<li>第二，<strong>每个分类属性的最大数量不能超过32个</strong>，如果属性超过32个，那么在使用randomForest()之前那些属性必须被转化。也可以通过另外一个包’cforest’建立随机森林，并且这个包里面的函数并不受属性的最大数量约束，尽管如此，高维的分类属性会使得它在建立随机森林的时候消耗大量的内存和时间。<h2 id="3-2-数据集拆分"><a href="#3-2-数据集拆分" class="headerlink" title="3.2 数据集拆分"></a>3.2 数据集拆分</h2><figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">ind &lt;- sample(<span class="number">2</span>, nrow(iris), replace=<span class="literal">TRUE</span>, prob=<span class="built_in">c</span>(<span class="number">0.7</span>, <span class="number">0.3</span>))</span><br><span class="line">trainData &lt;- iris[ind==<span class="number">1</span>,]</span><br><span class="line">testData &lt;- iris[ind==<span class="number">2</span>,]</span><br></pre></td></tr></table></figure>
<img src="https://img-blog.csdnimg.cn/e6e2ec818289409b9616ed0f394e5146.png" alt="在这里插入图片描述"><h2 id="3-3-随机森林建模"><a href="#3-3-随机森林建模" class="headerlink" title="3.3 随机森林建模"></a>3.3 随机森林建模</h2><figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">rf &lt;- randomForest(Species ~ ., data=trainData, ntree=<span class="number">100</span>, proximity=<span class="literal">TRUE</span>)</span><br><span class="line"><span class="comment">#Species ~ .指的是Species与其他所有属性之间的等式</span></span><br><span class="line"><span class="comment"># ntree：指定随机森林所包含的决策树数目，默认为500</span></span><br><span class="line"><span class="comment"># proximity：逻辑参数，是否计算模型的临近矩阵，主要结合MDSplot()函数使用</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#查看预测结果</span></span><br><span class="line">table(predict(rf), trainData$Species)</span><br></pre></td></tr></table></figure>
<img src="https://img-blog.csdnimg.cn/fcde7eac24cd4624a6944876db6bfdd1.png" alt="在这里插入图片描述"><br>由上图的结果可知，即使在决策树中，仍然有误差，第二类和第三类话仍然会被误判<figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#查看模型</span></span><br><span class="line">print(rf)</span><br></pre></td></tr></table></figure>
<img src="https://img-blog.csdnimg.cn/9f82cf3acd2b44889cf08c0d47f30ef1.png" alt="在这里插入图片描述"><br>通过输入print(rf)知道<strong>误判率为3.81%</strong><figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#绘制每一棵树的误判率的图</span></span><br><span class="line">plot(rf)</span><br></pre></td></tr></table></figure>
<img src="https://img-blog.csdnimg.cn/828091b670cc4ba3b250b3a453208cfa.png" alt="在这里插入图片描述"><br>可以通过输入plot(rf)绘制每一棵树的误判率的图</li>
</ul>
<h2 id="3-4-测试集预测"><a href="#3-4-测试集预测" class="headerlink" title="3.4 测试集预测"></a>3.4 测试集预测</h2><p>最后，在测试集上测试训练集上建立的随机森林，并使用table()和margin()函数检测预测结果。</p>
<figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">irisPred &lt;- predict(rf, newdata=testData)</span><br><span class="line"></span><br><span class="line"><span class="comment">#查看测试集预测结果</span></span><br><span class="line">table(irisPred, testData$Species)</span><br></pre></td></tr></table></figure>
<p><img src="https://img-blog.csdnimg.cn/a4fb5e05fe1d4240ab7f05d6b286c18d.png" alt="在这里插入图片描述"></p>
<h2 id="3-5-绘制概率图"><a href="#3-5-绘制概率图" class="headerlink" title="3.5 绘制概率图"></a>3.5 绘制概率图</h2><figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#绘制每一个观测值被判断正确的概率图</span></span><br><span class="line">plot(margin(rf, testData$Species))</span><br></pre></td></tr></table></figure>
<p><img src="https://img-blog.csdnimg.cn/76b54e7a56ee440a8bb35254c0560eda.png" alt="在这里插入图片描述"></p>
<h1 id="实验分析"><a href="#实验分析" class="headerlink" title="实验分析"></a>实验分析</h1><ul>
<li>由测试集结果可知，ctree()的目前版本（0.9～9995）<strong>不能很好的处理缺失值</strong>，因此含有缺失值的实例有时会被划分到左子树中，有时会被划分到右子树中，这是由替代规则决定的。</li>
<li>如果训练集中的一个变量在使用函数 ctree()<strong>构建决策树后</strong>被剔除，那么在对测试集进行预测时也必须包含该变量，否则调用函数 predict()会失败。</li>
<li>如果测试集与实训集对<strong>分类变量水平值</strong>不同，对测试集对预测也会失败。解决此问题的另一个方法是，使用训练集构建一棵决策树后，再利用第一课决策树中包含的所有变量<strong>重新调用 ctree()建立一棵新的决策树</strong>，并根据测试集中分类变量的水平值显示的设置训练数据。</li>
</ul>
<h1 id="资料"><a href="#资料" class="headerlink" title="资料"></a>资料</h1><p><a target="_blank" rel="noopener" href="https://www.aliyundrive.com/s/sNELP1sAj6y">实验资料/阿里云盘</a></p>
<p><a target="_blank" rel="noopener" href="https://pan.baidu.com/s/13DvGNLGSgSHXonPxHOPGVw?pwd=h030">程序包/百度网盘</a></p>
<p>提取码：h030（无需提取码，以防万一）</p>
<h1 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h1><p><a target="_blank" rel="noopener" href="https://www.cnblogs.com/jpld/p/4868761.html">【R语言进行数据挖掘】决策树和随机森林/文博客园@tjxj666</a></p>
<p><a target="_blank" rel="noopener" href="https://www.cda.cn/view/125822.html">决策树与随机森林的R语言实现/文数据分析师</a></p>
<p><a target="_blank" rel="noopener" href="https://www.cnblogs.com/karlpearson/p/6224148.html">决策树及R语言实现/文博客园@数学男</a></p>
<p><a target="_blank" rel="noopener" href="https://blog.csdn.net/tarim/article/details/41293193">决策树与R语言(RPART)/文CSDN@tarim</a></p>
<p><a target="_blank" rel="noopener" href="https://blog.csdn.net/LdyInG_/article/details/103533025">R语言笔记：机器学习【决策树（Decision Tree】/文CSDN@LdyInG_</a></p>
<p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/24416833">Learn R | Random Forest of Data Mining（下）/文知乎@Jason</a></p>
<p><a target="_blank" rel="noopener" href="https://developer.aliyun.com/article/117520">《R语言与数据挖掘最佳实践和经典案例》—— 1.3　数据集/文阿里云开发者社区@华章出版社</a></p>
<p><a target="_blank" rel="noopener" href="https://blog.csdn.net/z120379372/article/details/77982999">分类-回归树模型（CART）在R语言中的实现/文CSDN@周小馬</a></p>
<p><a target="_blank" rel="noopener" href="https://www.cnblogs.com/amengduo/p/9587260.html">分类-回归树模型（CART）在R语言中的实现/文博客园@刘小子</a></p>
<p><a target="_blank" rel="noopener" href="https://www.jianshu.com/p/6ada344f91ce">鸢尾花数据集/文简书@littlehei</a></p>
<p><a target="_blank" rel="noopener" href="https://www.jianshu.com/p/20bb2528753b">R语言sample()函数/文简书@tianzhanlan</a></p>
<h1 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h1><ul>
<li>个人感觉<strong>函数直接看官方文档效率会高一些</strong>，技术社区里常用的函数会有些人写一下注释，冷门函数看官方文档比较好</li>
<li>技术社区<strong>常用技术</strong>水文较多，一错全错</li>
</ul>

  <p><a class="classtest-link" href="/tags/Data-Mining/" rel="tag">Data Mining</a>, <a class="classtest-link" href="/tags/%E5%AE%9E%E9%AA%8C/" rel="tag">实验</a> — 2022年4月19日</p>
  


        </div>
        <div class="row mt-2">
  <h3>Search</h3>
  <div><input id="search-text" title="search" class="search-text" type="text" placeholder="search......"></div>
  <div style="margin-top: 1.5rem;">
    <ul id="result"></ul>
  </div>
</div>
        <div class="row mt-2">
  
    <div class="eight columns">
      <p id="madewith">Made with ❤ and
        <a class="footer-link icon" href="https://hexo.io" target="_blank" style="text-decoration: none;" rel="noreferrer" aria-label="Hexo.io">
        <svg class="hexo svg-hov" width="14" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><title>Hexo.js</title><path d="M12 .007L1.57 6.056V18.05L12 23.995l10.43-6.049V5.952L12 .007zm4.798 17.105l-.939.521-.939-.521V12.94H9.08v4.172l-.94.521-.938-.521V6.89l.939-.521.939.521v4.172h5.84V6.89l.94-.521.938.521v10.222z"/></svg>
        </a>
        
        at <a href="https://en.wikipedia.org/wiki/Earth" target="_blank" rel="noreferrer">Earth</a>.</p>
        
    </div>

    <!-- Sepcial thanks to https://simpleicons.org/ for the icons -->
    <div class="four columns mb-3 posisi" >
      
      <a class="ml-0 footer-link icon" href="https://github.com/adisaktijrs" target="_blank" style="text-decoration: none" rel="noreferrer" aria-label="GitHub">
        <svg class="github svg-hov" width="18" role="img" viewBox="0 0 24 24" xmlns="http://www.w3.org/2000/svg"><title>GitHub</title><path d="M12 .297c-6.63 0-12 5.373-12 12 0 5.303 3.438 9.8 8.205 11.385.6.113.82-.258.82-.577 0-.285-.01-1.04-.015-2.04-3.338.724-4.042-1.61-4.042-1.61C4.422 18.07 3.633 17.7 3.633 17.7c-1.087-.744.084-.729.084-.729 1.205.084 1.838 1.236 1.838 1.236 1.07 1.835 2.809 1.305 3.495.998.108-.776.417-1.305.76-1.605-2.665-.3-5.466-1.332-5.466-5.93 0-1.31.465-2.38 1.235-3.22-.135-.303-.54-1.523.105-3.176 0 0 1.005-.322 3.3 1.23.96-.267 1.98-.399 3-.405 1.02.006 2.04.138 3 .405 2.28-1.552 3.285-1.23 3.285-1.23.645 1.653.24 2.873.12 3.176.765.84 1.23 1.91 1.23 3.22 0 4.61-2.805 5.625-5.475 5.92.42.36.81 1.096.81 2.22 0 1.606-.015 2.896-.015 3.286 0 .315.21.69.825.57C20.565 22.092 24 17.592 24 12.297c0-6.627-5.373-12-12-12"/></svg>
      </a>
      

      
      <a class="ml-0 footer-link icon" href="https://linkedin.com/in/adisaktijrs" target="_blank" style="text-decoration: none" rel="noreferrer" aria-label="LinkedIn">
        <svg class="linkedin svg-hov" width="18" role="img" viewBox="0 0 24 24" xmlns="http://www.w3.org/2000/svg"><title>LinkedIn</title><path d="M20.447 20.452h-3.554v-5.569c0-1.328-.027-3.037-1.852-3.037-1.853 0-2.136 1.445-2.136 2.939v5.667H9.351V9h3.414v1.561h.046c.477-.9 1.637-1.85 3.37-1.85 3.601 0 4.267 2.37 4.267 5.455v6.286zM5.337 7.433c-1.144 0-2.063-.926-2.063-2.065 0-1.138.92-2.063 2.063-2.063 1.14 0 2.064.925 2.064 2.063 0 1.139-.925 2.065-2.064 2.065zm1.782 13.019H3.555V9h3.564v11.452zM22.225 0H1.771C.792 0 0 .774 0 1.729v20.542C0 23.227.792 24 1.771 24h20.451C23.2 24 24 23.227 24 22.271V1.729C24 .774 23.2 0 22.222 0h.003z"/></svg>
      </a>
      

      
      <a class="ml-0 footer-link icon" href="https://twitter.com/adisaktijrs" target="_blank" style="text-decoration: none" rel="noreferrer" aria-label="Twitter">
        <svg class="twitter svg-hov" width="18" role="img" viewBox="0 0 24 24" xmlns="http://www.w3.org/2000/svg"><title>Twitter</title><path d="M23.954 4.569c-.885.389-1.83.654-2.825.775 1.014-.611 1.794-1.574 2.163-2.723-.951.555-2.005.959-3.127 1.184-.896-.959-2.173-1.559-3.591-1.559-2.717 0-4.92 2.203-4.92 4.917 0 .39.045.765.127 1.124C7.691 8.094 4.066 6.13 1.64 3.161c-.427.722-.666 1.561-.666 2.475 0 1.71.87 3.213 2.188 4.096-.807-.026-1.566-.248-2.228-.616v.061c0 2.385 1.693 4.374 3.946 4.827-.413.111-.849.171-1.296.171-.314 0-.615-.03-.916-.086.631 1.953 2.445 3.377 4.604 3.417-1.68 1.319-3.809 2.105-6.102 2.105-.39 0-.779-.023-1.17-.067 2.189 1.394 4.768 2.209 7.557 2.209 9.054 0 13.999-7.496 13.999-13.986 0-.209 0-.42-.015-.63.961-.689 1.8-1.56 2.46-2.548l-.047-.02z"/></svg>
      </a>
      

      
      <a class="ml-0 footer-link icon" href="https://instagram.com/adisaktijrs" target="_blank" style="text-decoration: none" rel="noreferrer" aria-label="Instagram">
        <svg class="instagram svg-hov" width="18" role="img" viewBox="0 0 24 24" xmlns="http://www.w3.org/2000/svg"><title>Instagram</title><path d="M12 0C8.74 0 8.333.015 7.053.072 5.775.132 4.905.333 4.14.63c-.789.306-1.459.717-2.126 1.384S.935 3.35.63 4.14C.333 4.905.131 5.775.072 7.053.012 8.333 0 8.74 0 12s.015 3.667.072 4.947c.06 1.277.261 2.148.558 2.913.306.788.717 1.459 1.384 2.126.667.666 1.336 1.079 2.126 1.384.766.296 1.636.499 2.913.558C8.333 23.988 8.74 24 12 24s3.667-.015 4.947-.072c1.277-.06 2.148-.262 2.913-.558.788-.306 1.459-.718 2.126-1.384.666-.667 1.079-1.335 1.384-2.126.296-.765.499-1.636.558-2.913.06-1.28.072-1.687.072-4.947s-.015-3.667-.072-4.947c-.06-1.277-.262-2.149-.558-2.913-.306-.789-.718-1.459-1.384-2.126C21.319 1.347 20.651.935 19.86.63c-.765-.297-1.636-.499-2.913-.558C15.667.012 15.26 0 12 0zm0 2.16c3.203 0 3.585.016 4.85.071 1.17.055 1.805.249 2.227.415.562.217.96.477 1.382.896.419.42.679.819.896 1.381.164.422.36 1.057.413 2.227.057 1.266.07 1.646.07 4.85s-.015 3.585-.074 4.85c-.061 1.17-.256 1.805-.421 2.227-.224.562-.479.96-.899 1.382-.419.419-.824.679-1.38.896-.42.164-1.065.36-2.235.413-1.274.057-1.649.07-4.859.07-3.211 0-3.586-.015-4.859-.074-1.171-.061-1.816-.256-2.236-.421-.569-.224-.96-.479-1.379-.899-.421-.419-.69-.824-.9-1.38-.165-.42-.359-1.065-.42-2.235-.045-1.26-.061-1.649-.061-4.844 0-3.196.016-3.586.061-4.861.061-1.17.255-1.814.42-2.234.21-.57.479-.96.9-1.381.419-.419.81-.689 1.379-.898.42-.166 1.051-.361 2.221-.421 1.275-.045 1.65-.06 4.859-.06l.045.03zm0 3.678c-3.405 0-6.162 2.76-6.162 6.162 0 3.405 2.76 6.162 6.162 6.162 3.405 0 6.162-2.76 6.162-6.162 0-3.405-2.76-6.162-6.162-6.162zM12 16c-2.21 0-4-1.79-4-4s1.79-4 4-4 4 1.79 4 4-1.79 4-4 4zm7.846-10.405c0 .795-.646 1.44-1.44 1.44-.795 0-1.44-.646-1.44-1.44 0-.794.646-1.439 1.44-1.439.793-.001 1.44.645 1.44 1.439z"/></svg>
      </a>
      

      
      <a class="ml-0 footer-link icon" href="https://stackoverflow.com/story/tobiasreithmeier" target="_blank" style="text-decoration: none" rel="noreferrer" aria-label="StackOverflow">
        <svg class="stackoverflow svg-hov" width="18" role="img" viewBox="0 0 24 24" xmlns="http://www.w3.org/2000/svg"><title>Stack Overflow</title><path d="M15.725 0l-1.72 1.277 6.39 8.588 1.716-1.277L15.725 0zm-3.94 3.418l-1.369 1.644 8.225 6.85 1.369-1.644-8.225-6.85zm-3.15 4.465l-.905 1.94 9.702 4.517.904-1.94-9.701-4.517zm-1.85 4.86l-.44 2.093 10.473 2.201.44-2.092-10.473-2.203zM1.89 15.47V24h19.19v-8.53h-2.133v6.397H4.021v-6.396H1.89zm4.265 2.133v2.13h10.66v-2.13H6.154Z"/></svg>
      </a>
      

    </div>
  
</div>

      </div>

    </div>

  </div>
  <script src="/js/nanobar.min.js"></script>

  <script>
    var options = {
      classname: 'nanobar',
      id: 'myNanobar'
    };
    var nanobar = new Nanobar(options);
    nanobar.go(30);
    nanobar.go(76);
    nanobar.go(100);
  </script>

</body>

</html>